{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2603715,"sourceType":"datasetVersion","datasetId":1582403},{"sourceId":8684998,"sourceType":"datasetVersion","datasetId":5207001},{"sourceId":94284,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":79039,"modelId":103533}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U pennylane==0.34.0 tensorflow==2.14.0 numpy scipy plotly seaborn pennylane-qiskit pylatexenc silence-tensorflow\n# pip install pennylane qiskit qiskit_machine_learning imblearn plotly seaborn pennylane-qiskit pylatexenc silence-tensorflow\n\nfrom IPython.core.display import HTML\nHTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\nfrom silence_tensorflow import silence_tensorflow\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import davies_bouldin_score, pairwise_distances, silhouette_samples, silhouette_score\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.manifold import TSNE\n\n# classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nfrom time import time\nfrom datetime import datetime\nimport matplotlib.cm as cm\nfrom scipy.spatial.distance import cdist\n\nfrom datetime import datetime\nfrom joblib import Parallel, delayed\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nimport seaborn as sns\nimport jax;\n\njax.config.update('jax_platform_name', 'cpu')\njax.config.update(\"jax_enable_x64\", True)\nimport jax.numpy as jnp\n\nimport optax  # optimization using jax\n\nimport pennylane as qml\nimport pennylane.numpy as pnp\n\nsns.set()\n\nseed = 0\nrng = np.random.default_rng(seed=seed)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o3DpTN8DP-lN","outputId":"9abaa642-866f-4fa0-86c2-40e29c6a0aa5","scrolled":true,"execution":{"iopub.status.busy":"2024-08-13T18:14:55.007720Z","iopub.execute_input":"2024-08-13T18:14:55.008821Z","iopub.status.idle":"2024-08-13T18:17:42.998436Z","shell.execute_reply.started":"2024-08-13T18:14:55.008786Z","shell.execute_reply":"2024-08-13T18:17:42.997555Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pennylane==0.34.0\n  Downloading PennyLane-0.34.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting tensorflow==2.14.0\n  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nCollecting numpy\n  Downloading numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m870.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\nCollecting scipy\n  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.18.0)\nCollecting plotly\n  Downloading plotly-5.23.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\nCollecting seaborn\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nCollecting pennylane-qiskit\n  Downloading PennyLane_qiskit-0.37.0-py3-none-any.whl.metadata (7.0 kB)\nCollecting pylatexenc\n  Downloading pylatexenc-2.10.tar.gz (162 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting silence-tensorflow\n  Downloading silence_tensorflow-1.2.2.tar.gz (5.3 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pennylane==0.34.0) (3.2.1)\nCollecting rustworkx (from pennylane==0.34.0)\n  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nCollecting autograd (from pennylane==0.34.0)\n  Downloading autograd-1.6.2-py3-none-any.whl.metadata (706 bytes)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from pennylane==0.34.0) (0.10.2)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pennylane==0.34.0) (1.4.4)\nCollecting semantic-version>=2.7 (from pennylane==0.34.0)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting autoray>=0.6.1 (from pennylane==0.34.0)\n  Downloading autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from pennylane==0.34.0) (4.2.4)\nCollecting pennylane-lightning>=0.34 (from pennylane==0.34.0)\n  Downloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (23 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pennylane==0.34.0) (2.32.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pennylane==0.34.0) (4.9.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (16.0.6)\nRequirement already satisfied: ml-dtypes==0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (2.4.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.14.0) (1.59.3)\nCollecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.0)\n  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\nCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.0)\n  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting keras<2.15,>=2.14.0 (from tensorflow==2.14.0)\n  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.2.3)\nRequirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.2.1)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.10/site-packages (from seaborn) (3.7.5)\nCollecting qiskit>=0.32 (from pennylane-qiskit)\n  Downloading qiskit-1.1.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting qiskit-aer (from pennylane-qiskit)\n  Downloading qiskit_aer-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\nCollecting qiskit-ibm-provider (from pennylane-qiskit)\n  Downloading qiskit_ibm_provider-0.11.0-py3-none-any.whl.metadata (7.6 kB)\nCollecting qiskit-ibm-runtime (from pennylane-qiskit)\n  Downloading qiskit_ibm_runtime-0.27.1-py3-none-any.whl.metadata (19 kB)\nINFO: pip is looking at multiple versions of pennylane-qiskit to determine which version is compatible with other requirements. This could take a while.\nCollecting pennylane-qiskit\n  Downloading PennyLane_qiskit-0.36.0-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.42.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.4)\nINFO: pip is looking at multiple versions of pennylane-lightning to determine which version is compatible with other requirements. This could take a while.\nCollecting pennylane-lightning>=0.34 (from pennylane==0.34.0)\n  Downloading PennyLane_Lightning-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: sympy>=1.3 in /opt/conda/lib/python3.10/site-packages (from qiskit>=0.32->pennylane-qiskit) (1.12.1)\nRequirement already satisfied: dill>=0.3 in /opt/conda/lib/python3.10/site-packages (from qiskit>=0.32->pennylane-qiskit) (0.3.8)\nCollecting stevedore>=3.0.0 (from qiskit>=0.32->pennylane-qiskit)\n  Downloading stevedore-5.2.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting symengine>=0.11 (from qiskit>=0.32->pennylane-qiskit)\n  Downloading symengine-0.11.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.2 kB)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.26.1)\nCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14.0)\n  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane==0.34.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane==0.34.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane==0.34.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane==0.34.0) (2024.2.2)\nRequirement already satisfied: future>=0.15.2 in /opt/conda/lib/python3.10/site-packages (from autograd->pennylane==0.34.0) (1.0.0)\nRequirement already satisfied: psutil>=5 in /opt/conda/lib/python3.10/site-packages (from qiskit-aer->pennylane-qiskit) (5.9.3)\nCollecting requests-ntlm>=1.1.0 (from qiskit-ibm-provider->pennylane-qiskit)\n  Downloading requests_ntlm-1.3.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: websocket-client>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-provider->pennylane-qiskit) (1.7.0)\nRequirement already satisfied: websockets>=10.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-provider->pennylane-qiskit) (12.0)\nCollecting ibm-platform-services>=0.22.6 (from qiskit-ibm-runtime->pennylane-qiskit)\n  Downloading ibm_platform_services-0.56.0-py3-none-any.whl.metadata (9.1 kB)\nRequirement already satisfied: pydantic>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from qiskit-ibm-runtime->pennylane-qiskit) (2.5.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.3.1)\nCollecting ibm-cloud-sdk-core<4.0.0,>=3.20.6 (from ibm-platform-services>=0.22.6->qiskit-ibm-runtime->pennylane-qiskit)\n  Downloading ibm_cloud_sdk_core-3.20.6-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime->pennylane-qiskit) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.5.0->qiskit-ibm-runtime->pennylane-qiskit) (2.14.6)\nRequirement already satisfied: cryptography>=1.3 in /opt/conda/lib/python3.10/site-packages (from requests-ntlm>=1.1.0->qiskit-ibm-provider->pennylane-qiskit) (41.0.7)\nCollecting pyspnego>=0.4.0 (from requests-ntlm>=1.1.0->qiskit-ibm-provider->pennylane-qiskit)\n  Downloading pyspnego-0.11.1-py3-none-any.whl.metadata (5.4 kB)\nCollecting pbr!=2.1.0,>=2.0.0 (from stevedore>=3.0.0->qiskit>=0.32->pennylane-qiskit)\n  Downloading pbr-6.0.0-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy>=1.3->qiskit>=0.32->pennylane-qiskit) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.1.3)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-provider->pennylane-qiskit) (1.16.0)\nCollecting urllib3<3,>=1.21.1 (from requests->pennylane==0.34.0)\n  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: PyJWT<3.0.0,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from ibm-cloud-sdk-core<4.0.0,>=3.20.6->ibm-platform-services>=0.22.6->qiskit-ibm-runtime->pennylane-qiskit) (2.8.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibm-provider->pennylane-qiskit) (2.21)\nDownloading PennyLane-0.34.0-py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading plotly-5.23.0-py3-none-any.whl (17.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PennyLane_qiskit-0.36.0-py3-none-any.whl (37 kB)\nDownloading autoray-0.6.12-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading PennyLane_Lightning-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading qiskit-1.1.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autograd-1.6.2-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qiskit_aer-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_provider-0.11.0-py3-none-any.whl (249 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qiskit_ibm_runtime-0.27.1-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nDownloading ibm_platform_services-0.56.0-py3-none-any.whl (342 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.7/342.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_ntlm-1.3.0-py3-none-any.whl (6.6 kB)\nDownloading stevedore-5.2.0-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading symengine-0.11.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (39.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ibm_cloud_sdk_core-3.20.6-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyspnego-0.11.1-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pylatexenc, silence-tensorflow\n  Building wheel for pylatexenc (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=fdc97851b1ef55fb29c8f6eb3ed1b03b416de11a3c16d65046ea3f52f8bad38f\n  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n  Building wheel for silence-tensorflow (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for silence-tensorflow: filename=silence_tensorflow-1.2.2-py3-none-any.whl size=5829 sha256=1cbedf2bbdd721ae0105f74d91d21727c5ac6481bc690cfac9ad89573336112d\n  Stored in directory: /root/.cache/pip/wheels/4e/e1/85/e501d21f8d79832caa65e1b779bc17d1847ebc0170f7bc5a80\nSuccessfully built pylatexenc silence-tensorflow\nInstalling collected packages: silence-tensorflow, pylatexenc, urllib3, tensorflow-estimator, symengine, semantic-version, scipy, rustworkx, pbr, keras, autoray, autograd, stevedore, plotly, seaborn, qiskit, pyspnego, ibm-cloud-sdk-core, requests-ntlm, qiskit-aer, ibm-platform-services, google-auth-oauthlib, tensorboard, qiskit-ibm-runtime, qiskit-ibm-provider, tensorflow, pennylane-lightning, pennylane, pennylane-qiskit\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.15.0\n    Uninstalling tensorflow-estimator-2.15.0:\n      Successfully uninstalled tensorflow-estimator-2.15.0\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.4\n    Uninstalling scipy-1.11.4:\n      Successfully uninstalled scipy-1.11.4\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n  Attempting uninstall: plotly\n    Found existing installation: plotly 5.18.0\n    Uninstalling plotly-5.18.0:\n      Successfully uninstalled plotly-5.18.0\n  Attempting uninstall: seaborn\n    Found existing installation: seaborn 0.12.2\n    Uninstalling seaborn-0.12.2:\n      Successfully uninstalled seaborn-0.12.2\n  Attempting uninstall: google-auth-oauthlib\n    Found existing installation: google-auth-oauthlib 1.2.0\n    Uninstalling google-auth-oauthlib-1.2.0:\n      Successfully uninstalled google-auth-oauthlib-1.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.14.0 which is incompatible.\ntensorflow-serving-api 2.14.1 requires tensorflow<3,>=2.14.1, but you have tensorflow 2.14.0 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.14.0 which is incompatible.\ntf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.14.0 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires scipy<1.12,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\nydata-profiling 4.6.4 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed autograd-1.6.2 autoray-0.6.12 google-auth-oauthlib-1.0.0 ibm-cloud-sdk-core-3.20.6 ibm-platform-services-0.56.0 keras-2.14.0 pbr-6.0.0 pennylane-0.34.0 pennylane-lightning-0.36.0 pennylane-qiskit-0.36.0 plotly-5.23.0 pylatexenc-2.10 pyspnego-0.11.1 qiskit-1.1.2 qiskit-aer-0.14.2 qiskit-ibm-provider-0.11.0 qiskit-ibm-runtime-0.27.1 requests-ntlm-1.3.0 rustworkx-0.15.1 scipy-1.13.1 seaborn-0.13.2 semantic-version-2.10.0 silence-tensorflow-1.2.2 stevedore-5.2.0 symengine-0.11.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 urllib3-2.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2024-08-13T18:17:43.000125Z","iopub.execute_input":"2024-08-13T18:17:43.000842Z","iopub.status.idle":"2024-08-13T18:17:43.006222Z","shell.execute_reply.started":"2024-08-13T18:17:43.000785Z","shell.execute_reply":"2024-08-13T18:17:43.005238Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pennylane as qml\nimport qiskit\nprint(qml.__version__)\nprint(qiskit.__version__)\nimport pennylane_qiskit\nprint(pennylane_qiskit.__version__)\nimport tensorflow as tf\nprint(tf.__version__)\n\n\n# Check if GPU is available\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT available\")\n\n# Check which GPU TensorFlow is using\nprint(\"TensorFlow is using\", tf.test.gpu_device_name())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5pYTkB5gZvtH","outputId":"699f0616-c84f-45e9-df48-87583336d07f","execution":{"iopub.status.busy":"2024-08-13T18:17:43.007660Z","iopub.execute_input":"2024-08-13T18:17:43.008241Z","iopub.status.idle":"2024-08-13T18:17:51.254473Z","shell.execute_reply.started":"2024-08-13T18:17:43.008215Z","shell.execute_reply":"2024-08-13T18:17:51.253346Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"0.34.0\n1.1.2\n0.36.0\n","output_type":"stream"},{"name":"stderr","text":"2024-08-13 18:17:45.397335: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-13 18:17:45.397418: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-13 18:17:45.397475: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"2.14.0\nGPU is NOT available\nTensorFlow is using \n","output_type":"stream"}]},{"cell_type":"code","source":"# prompt: import labels.csv and features.csv and concat them into df\n\nimport pandas as pd\n# labels_df = pd.read_csv('/content/labels.csv')\n# features_df = pd.read_csv('/content/features.csv')\n\n# df = pd.concat([features_df, labels_df], axis=1)\ndf = pd.read_csv('/kaggle/input/heart-failure-prediction/heart.csv')\ndf.head()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"PZz-Hx0tWMfl","outputId":"ea6a3454-ece6-402a-b32b-cd6295580e78","execution":{"iopub.status.busy":"2024-08-13T18:17:51.256640Z","iopub.execute_input":"2024-08-13T18:17:51.257341Z","iopub.status.idle":"2024-08-13T18:17:51.319468Z","shell.execute_reply.started":"2024-08-13T18:17:51.257314Z","shell.execute_reply":"2024-08-13T18:17:51.318326Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n0   40   M           ATA        140          289          0     Normal    172   \n1   49   F           NAP        160          180          0     Normal    156   \n2   37   M           ATA        130          283          0         ST     98   \n3   48   F           ASY        138          214          0     Normal    108   \n4   54   M           NAP        150          195          0     Normal    122   \n\n  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n0              N      0.0       Up             0  \n1              N      1.0     Flat             1  \n2              N      0.0       Up             0  \n3              Y      1.5     Flat             1  \n4              N      0.0       Up             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n      <th>HeartDisease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>140</td>\n      <td>289</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>172</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>F</td>\n      <td>NAP</td>\n      <td>160</td>\n      <td>180</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>156</td>\n      <td>N</td>\n      <td>1.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>283</td>\n      <td>0</td>\n      <td>ST</td>\n      <td>98</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>F</td>\n      <td>ASY</td>\n      <td>138</td>\n      <td>214</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>108</td>\n      <td>Y</td>\n      <td>1.5</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>150</td>\n      <td>195</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>122</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQ7_gy6MP-lp","outputId":"cc2b27f9-8b2d-4a13-a2e1-3cf319d483f7","execution":{"iopub.status.busy":"2024-08-13T18:17:51.320608Z","iopub.execute_input":"2024-08-13T18:17:51.320939Z","iopub.status.idle":"2024-08-13T18:17:51.327556Z","shell.execute_reply.started":"2024-08-13T18:17:51.320912Z","shell.execute_reply":"2024-08-13T18:17:51.326473Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(918, 12)"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w8V1MnyjV2OP","outputId":"8148914f-c64a-4880-f0c5-ca3938950872","execution":{"iopub.status.busy":"2024-08-13T18:17:51.329434Z","iopub.execute_input":"2024-08-13T18:17:51.329863Z","iopub.status.idle":"2024-08-13T18:17:51.368008Z","shell.execute_reply.started":"2024-08-13T18:17:51.329826Z","shell.execute_reply":"2024-08-13T18:17:51.366796Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 918 entries, 0 to 917\nData columns (total 12 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Age             918 non-null    int64  \n 1   Sex             918 non-null    object \n 2   ChestPainType   918 non-null    object \n 3   RestingBP       918 non-null    int64  \n 4   Cholesterol     918 non-null    int64  \n 5   FastingBS       918 non-null    int64  \n 6   RestingECG      918 non-null    object \n 7   MaxHR           918 non-null    int64  \n 8   ExerciseAngina  918 non-null    object \n 9   Oldpeak         918 non-null    float64\n 10  ST_Slope        918 non-null    object \n 11  HeartDisease    918 non-null    int64  \ndtypes: float64(1), int64(6), object(5)\nmemory usage: 86.2+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Columns to encode\ncolumns_to_encode = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n\n# Initialize label encoder\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoders = {}\n\n# Apply label encoding to each column\nfor column in columns_to_encode:\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column])\n    label_encoders[column] = le  # Save the encoder to inverse transform if needed later\n","metadata":{"id":"GbnK4PRgxxjd","execution":{"iopub.status.busy":"2024-08-13T18:17:51.369335Z","iopub.execute_input":"2024-08-13T18:17:51.369685Z","iopub.status.idle":"2024-08-13T18:17:51.384866Z","shell.execute_reply.started":"2024-08-13T18:17:51.369657Z","shell.execute_reply":"2024-08-13T18:17:51.384050Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LwAKjr3x7Ei","outputId":"4b4e07d4-869c-464a-c0aa-58e9950704e9","execution":{"iopub.status.busy":"2024-08-13T18:17:51.386086Z","iopub.execute_input":"2024-08-13T18:17:51.386618Z","iopub.status.idle":"2024-08-13T18:17:51.406989Z","shell.execute_reply.started":"2024-08-13T18:17:51.386587Z","shell.execute_reply":"2024-08-13T18:17:51.406043Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Age                50\nSex                 2\nChestPainType       4\nRestingBP          67\nCholesterol       222\nFastingBS           2\nRestingECG          3\nMaxHR             119\nExerciseAngina      2\nOldpeak            53\nST_Slope            3\nHeartDisease        2\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming df is your DataFrame containing the dataset\n\n# Define X (features) and y (target variable)\nX = df.drop(columns=['HeartDisease'])  # Drop the target column\ny = df['HeartDisease']  # Target column\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Print the shapes of the resulting sets\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)\n\n\nsilence_tensorflow()\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport pennylane as qml\nimport tensorflow as tf\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\ntrainX = X_train\ntestX = X_test\ntrainy = y_train\ntesty = y_test\n\n# trainy = tf.one_hot(trainy, depth=2)\n# testy = tf.one_hot(testy, depth=2)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() #create an instance of the function\ntrainX = scaler.fit_transform(trainX) #fit and tranform training data\ntestX = scaler.transform(testX) #only tranform test data\n\ntrainX_reshaped = X_train.to_numpy().reshape(X_train.shape[0], X_train.shape[1], 1)\ntestX_reshaped = X_test.to_numpy().reshape(X_test.shape[0], X_test.shape[1], 1)\nprint(trainX_reshaped.shape, testX_reshaped.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OC_V4TqBV2Us","outputId":"c30d92b8-c385-4d13-9957-0005126935ba","execution":{"iopub.status.busy":"2024-08-13T18:17:51.408313Z","iopub.execute_input":"2024-08-13T18:17:51.409018Z","iopub.status.idle":"2024-08-13T18:17:51.435987Z","shell.execute_reply.started":"2024-08-13T18:17:51.408989Z","shell.execute_reply":"2024-08-13T18:17:51.435055Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"X_train shape: (642, 11)\nX_test shape: (276, 11)\ny_train shape: (642,)\ny_test shape: (276,)\n(642, 11, 1) (276, 11, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Proposed","metadata":{"id":"1gjT5_4yWypU"}},{"cell_type":"code","source":"# prompt: git clone\n!pip install pennylane silence_tensorflow keras-tuner\n!git clone https://github.com/ZPZhou-lab/tfkan.git\n!cd tfkan && pip install .","metadata":{"id":"zgeHiyyGXDkv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f4652d9-b596-4d3b-a332-0095d3496a19","scrolled":true,"execution":{"iopub.status.busy":"2024-08-13T18:17:51.439334Z","iopub.execute_input":"2024-08-13T18:17:51.439765Z","iopub.status.idle":"2024-08-13T18:18:24.858570Z","shell.execute_reply.started":"2024-08-13T18:17:51.439725Z","shell.execute_reply":"2024-08-13T18:18:24.857262Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pennylane in /opt/conda/lib/python3.10/site-packages (0.34.0)\nRequirement already satisfied: silence_tensorflow in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: keras-tuner in /opt/conda/lib/python3.10/site-packages (1.4.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.13.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (3.2.1)\nRequirement already satisfied: rustworkx in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.15.1)\nRequirement already satisfied: autograd in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.6.2)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.10.2)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from pennylane) (1.4.4)\nRequirement already satisfied: semantic-version>=2.7 in /opt/conda/lib/python3.10/site-packages (from pennylane) (2.10.0)\nRequirement already satisfied: autoray>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.6.12)\nRequirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.2.4)\nRequirement already satisfied: pennylane-lightning>=0.34 in /opt/conda/lib/python3.10/site-packages (from pennylane) (0.36.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pennylane) (4.9.0)\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (2.14.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (21.3)\nRequirement already satisfied: kt-legacy in /opt/conda/lib/python3.10/site-packages (from keras-tuner) (1.0.5)\nRequirement already satisfied: future>=0.15.2 in /opt/conda/lib/python3.10/site-packages (from autograd->pennylane) (1.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras-tuner) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->pennylane) (2024.2.2)\nCloning into 'tfkan'...\nremote: Enumerating objects: 81, done.\u001b[K\nremote: Counting objects: 100% (81/81), done.\u001b[K\nremote: Compressing objects: 100% (55/55), done.\u001b[K\nremote: Total 81 (delta 25), reused 71 (delta 18), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (81/81), 234.16 KiB | 2.32 MiB/s, done.\nProcessing /kaggle/working/tfkan\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorflow<=2.15.1,>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from tfkan==0.1.0) (2.14.0)\nRequirement already satisfied: keras<=2.15.1 in /opt/conda/lib/python3.10/site-packages (from tfkan==0.1.0) (2.14.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (16.0.6)\nRequirement already satisfied: ml-dtypes==0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (0.2.0)\nRequirement already satisfied: numpy>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (1.59.3)\nRequirement already satisfied: tensorboard<2.15,>=2.14 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (2.14.1)\nRequirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (2.14.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow<=2.15.1,>=2.10.0->tfkan==0.1.0) (3.2.2)\nBuilding wheels for collected packages: tfkan\n  Building wheel for tfkan (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tfkan: filename=tfkan-0.1.0-py3-none-any.whl size=12366 sha256=6f487e7ad32feb6e62aa1434ac4cc573f66bbf97ee963d2be41ff8ffc5e426f9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-5dji2iss/wheels/8b/9b/ad/a4e2ec558b8612078d6d55fc115466424a4491dd7a671448d3\nSuccessfully built tfkan\nInstalling collected packages: tfkan\nSuccessfully installed tfkan-0.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from tfkan import layers\nfrom tfkan.layers import DenseKAN, Conv2DKAN, Conv1DKAN\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import models, layers, optimizers, metrics\nimport pennylane as qml\nimport tensorflow as tf\nfrom silence_tensorflow import silence_tensorflow\nsilence_tensorflow()\ntf.keras.backend.set_floatx('float64')\ntf.keras.backend.clear_session()\n\n### essentials ###\nn_qubits = 4\nlayers = 1\ndata_dimension = 1  #len(YCOLS) #Y.shape[1]  ## output dimenstion according to one-hot encoding depth\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n## extras ###\nn_block_wires = 2\nn_params_block = 2\nn_blocks = qml.TTN.get_n_blocks(range(n_qubits), n_block_wires)\n\n@qml.qnode(dev)\ndef qnode(weights, inputs=None):\n    qml.templates.AmplitudeEmbedding(features=inputs, wires=range(n_qubits),normalize=True) # first turn it true, then false, again true. It'll work\n    #qml.templates.AngleEmbedding(features=inputs, wires=range(n_qubits), rotation='Z')\n    # for w in weights:\n    #     qml.TTN(range(n_qubits), n_block_wires, block, n_params_block, w)\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nweight_shapes = {\"weights\": (layers,n_qubits,3)}\n\nqlayer_1 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits, name='QuantumLayer1')\nqlayer_2 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits, name='QuantumLayer2')\nqlayer_3 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits, name='QuantumLayer3')","metadata":{"id":"xBoN2px-XDnj","execution":{"iopub.status.busy":"2024-08-13T21:30:15.585412Z","iopub.execute_input":"2024-08-13T21:30:15.585748Z","iopub.status.idle":"2024-08-13T21:30:15.620192Z","shell.execute_reply.started":"2024-08-13T21:30:15.585724Z","shell.execute_reply":"2024-08-13T21:30:15.618922Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# weights_init = 2*np.pi * np.random.randn(num_layers,n_blocks, n_params_block, requires_grad=True)\n# bias_init = np.array(0.0, requires_grad=True)\n# print(weights_init, bias_init)\n# circuit(weights_init,np.asarray(X_train.iloc[0]))\n# dev._circuit.draw('mpl', plot_barriers=True, reverse_bits=True)","metadata":{"id":"SrzHL8YZY7Ka","execution":{"iopub.status.busy":"2024-06-15T13:41:40.937928Z","iopub.execute_input":"2024-06-15T13:41:40.938358Z","iopub.status.idle":"2024-06-15T13:41:40.944455Z","shell.execute_reply.started":"2024-06-15T13:41:40.938322Z","shell.execute_reply":"2024-06-15T13:41:40.942852Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Model-1: BiLSTMKANnet","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom kerastuner import HyperModel\nfrom kerastuner.tuners import Hyperband\nfrom kerastuner.engine.hyperparameters import HyperParameters\n\nbatch_size = 10\n# Define the input layer\ninputs = layers.Input(shape=(trainX_reshaped.shape[1], 1))\n\nclass MyHyperModel(HyperModel):\n    def build(self, hp):\n        model1 = layers.Bidirectional(layers.LSTM(\n            hp.Int('units1', min_value=32, max_value=128, step=32),\n            activation='relu', return_sequences=True))(inputs)\n\n        model1 = layers.Bidirectional(layers.LSTM(\n            hp.Int('units2', min_value=16, max_value=64, step=16),\n            activation='relu', return_sequences=True))(model1)\n\n        model1 = layers.Flatten()(model1)\n        model1 = layers.Dropout(0.2)(model1)\n\n        model1 = layers.Dense(hp.Int('dense_units1', min_value=128, max_value=512, step=64), activation='relu')(model1)\n        model1 = layers.Dropout(0.2)(model1)\n\n        model1 = DenseKAN(units=hp.Int('units', min_value=32, max_value=256, step=32), grid_size=3)(model1)\n        model1 = layers.Dropout(0.2)(model1)\n\n        model1 = DenseKAN(32, grid_size=3)(model1)\n        model1 = layers.Dropout(0.2)(model1)\n\n        outputs = layers.Dense(data_dimension, activation='sigmoid')(model1)\n\n        model = models.Model(inputs=inputs, outputs=outputs)\n\n        learning_rate = 1e-3\n        optimizer = optimizers.Adam(learning_rate=learning_rate)\n        model.compile(\n            loss='binary_crossentropy',\n            optimizer=optimizer,\n            metrics='accuracy'\n        )\n\n        return model\n\n# Define the hypermodel\nhypermodel = MyHyperModel()\n\n# Initialize the tuner\ntuner = Hyperband(\n    hypermodel,\n    objective='val_accuracy',\n    max_epochs=10,\n    factor=3,\n    directory='my_dir',\n    project_name='hyperband_tuning'\n)\n\n# Print the results summary\ntuner.results_summary()\n\n# Search for the best hyperparameters\ntuner.search(trainX, trainy,\n             batch_size=batch_size,\n             epochs=50,\n             validation_data=(testX, testy),\n             callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n\n# Get the best hyperparameters and model\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nbest_model = tuner.hypermodel.build(best_hps)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rsXUnoB5-yli","outputId":"ae237958-2255-46c3-af1b-bb612af3e105","scrolled":true,"execution":{"iopub.status.busy":"2024-06-15T10:09:22.005490Z","iopub.execute_input":"2024-06-15T10:09:22.006234Z","iopub.status.idle":"2024-06-15T10:20:51.733146Z","shell.execute_reply.started":"2024-06-15T10:09:22.006196Z","shell.execute_reply":"2024-06-15T10:20:51.731871Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Trial 30 Complete [00h 00m 34s]\nval_accuracy: 0.8405797101449275\n\nBest val_accuracy So Far: 0.8695652173913043\nTotal elapsed time: 00h 11m 09s\n","output_type":"stream"}]},{"cell_type":"code","source":"tuner.results_summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-15T10:20:51.734816Z","iopub.execute_input":"2024-06-15T10:20:51.735179Z","iopub.status.idle":"2024-06-15T10:20:51.742654Z","shell.execute_reply.started":"2024-06-15T10:20:51.735146Z","shell.execute_reply":"2024-06-15T10:20:51.741446Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Results summary\nResults in my_dir/hyperband_tuning\nShowing 10 best trials\nObjective(name=\"val_accuracy\", direction=\"max\")\n\nTrial 0026 summary\nHyperparameters:\nunits1: 64\nunits2: 64\ndense_units1: 320\nunits: 256\ntuner/epochs: 10\ntuner/initial_epoch: 0\ntuner/bracket: 0\ntuner/round: 0\nScore: 0.8695652173913043\n\nTrial 0027 summary\nHyperparameters:\nunits1: 64\nunits2: 16\ndense_units1: 128\nunits: 128\ntuner/epochs: 10\ntuner/initial_epoch: 0\ntuner/bracket: 0\ntuner/round: 0\nScore: 0.8695652173913043\n\nTrial 0016 summary\nHyperparameters:\nunits1: 32\nunits2: 48\ndense_units1: 448\nunits: 192\ntuner/epochs: 10\ntuner/initial_epoch: 4\ntuner/bracket: 2\ntuner/round: 2\ntuner/trial_id: 0013\nScore: 0.8659420289855072\n\nTrial 0013 summary\nHyperparameters:\nunits1: 32\nunits2: 48\ndense_units1: 448\nunits: 192\ntuner/epochs: 4\ntuner/initial_epoch: 2\ntuner/bracket: 2\ntuner/round: 1\ntuner/trial_id: 0000\nScore: 0.8623188405797102\n\nTrial 0015 summary\nHyperparameters:\nunits1: 32\nunits2: 48\ndense_units1: 512\nunits: 64\ntuner/epochs: 4\ntuner/initial_epoch: 2\ntuner/bracket: 2\ntuner/round: 1\ntuner/trial_id: 0007\nScore: 0.8586956521739131\n\nTrial 0017 summary\nHyperparameters:\nunits1: 32\nunits2: 48\ndense_units1: 512\nunits: 64\ntuner/epochs: 10\ntuner/initial_epoch: 4\ntuner/bracket: 2\ntuner/round: 2\ntuner/trial_id: 0015\nScore: 0.8586956521739131\n\nTrial 0024 summary\nHyperparameters:\nunits1: 128\nunits2: 48\ndense_units1: 320\nunits: 128\ntuner/epochs: 10\ntuner/initial_epoch: 4\ntuner/bracket: 1\ntuner/round: 1\ntuner/trial_id: 0020\nScore: 0.8586956521739131\n\nTrial 0025 summary\nHyperparameters:\nunits1: 128\nunits2: 16\ndense_units1: 320\nunits: 256\ntuner/epochs: 10\ntuner/initial_epoch: 4\ntuner/bracket: 1\ntuner/round: 1\ntuner/trial_id: 0021\nScore: 0.855072463768116\n\nTrial 0028 summary\nHyperparameters:\nunits1: 64\nunits2: 48\ndense_units1: 128\nunits: 128\ntuner/epochs: 10\ntuner/initial_epoch: 0\ntuner/bracket: 0\ntuner/round: 0\nScore: 0.8478260869565217\n\nTrial 0029 summary\nHyperparameters:\nunits1: 96\nunits2: 48\ndense_units1: 320\nunits: 160\ntuner/epochs: 10\ntuner/initial_epoch: 0\ntuner/bracket: 0\ntuner/round: 0\nScore: 0.8405797101449275\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout,Lambda, LSTM\nfrom keras.initializers import random_uniform\nbatch_size = 10\n# Define the input layer\ninputs = layers.Input(shape=(trainX_reshaped.shape[1], 1))\n\nmodel1 = layers.Bidirectional(LSTM(64, activation='relu', return_sequences=True))(inputs)\nmodel1 = layers.Bidirectional(LSTM(64, activation='relu', return_sequences=True))(model1)\nmodel1 = layers.Flatten()(model1)\nmodel1 = Dropout(0.20)(model1)\nmodel1 = Dense(320, activation='relu')(model1)\nmodel1 = Dropout(0.20)(model1)\nmodel1 = DenseKAN(256, grid_size=3)(model1)\nmodel1 = Dropout(0.20)(model1)\nmodel1 = DenseKAN(32, grid_size=3)(model1)\nmodel1 = Dropout(0.20)(model1)\noutputs = layers.Dense(data_dimension, activation='sigmoid')(model1)\nmodel = models.Model(inputs=inputs, outputs=outputs)\n\n\nclass ReduceLRBacktrack(tf.keras.callbacks.ReduceLROnPlateau):\n    def __init__(self, best_path, *args, **kwargs):\n        super(ReduceLRBacktrack, self).__init__(*args, **kwargs)\n        self.best_path = best_path\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(self.monitor)\n        if current is None:\n            logging.warning('Reduce LR on plateau conditioned on metric `%s` '\n                            'which is not available. Available metrics are: %s',\n                             self.monitor, ','.join(list(logs.keys())))\n        if not self.monitor_op(current, self.best): # not new best\n            if not self.in_cooldown(): # and we're not in cooldown\n                if self.wait+1 >= self.patience: # going to reduce lr\n                    # load best model so far\n                    # print(\"\\n Backtracking to best model before reducting LR\")\n                    self.model.load_weights(self.best_path)\n\n        super().on_epoch_end(epoch, logs) # actually reduce LR\n\nmodel_path = 'bilstmkan_best.hdf5' # hdf5 for TF v2 2.14.0, for others h5 or keras or tf\n\nlr_reducer = ReduceLRBacktrack(\n    best_path = model_path,\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=5,\n    mode='min',\n    min_lr=1e-10\n)\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    model_path, save_weights_only=True, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1\n)\n\n# Compile the model\nlearning_rate = 1e-3\noptimizer = optimizers.Adam(learning_rate=learning_rate)\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\nfrom tensorflow.keras.metrics import BinaryAccuracy\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=optimizer,\n    metrics='accuracy'\n)\nhistory = model.fit(\n    trainX,\n    trainy,\n    # batch_size=batch_size,\n    epochs = 100,\n    callbacks=[checkpoint, lr_reducer, early_stopping],\n    validation_data = [testX, testy]\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-13T20:48:01.542248Z","iopub.execute_input":"2024-08-13T20:48:01.542660Z","iopub.status.idle":"2024-08-13T20:49:41.388438Z","shell.execute_reply.started":"2024-08-13T20:48:01.542628Z","shell.execute_reply":"2024-08-13T20:49:41.387539Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.7115 - accuracy: 0.5078\nEpoch 1: val_accuracy improved from -inf to 0.59420, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 10s 130ms/step - loss: 0.7114 - accuracy: 0.5078 - val_loss: 0.6776 - val_accuracy: 0.5942 - lr: 1.0000e-03\nEpoch 2/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.7165 - accuracy: 0.4719\nEpoch 2: val_accuracy did not improve from 0.59420\n21/21 [==============================] - 2s 78ms/step - loss: 0.7162 - accuracy: 0.4720 - val_loss: 0.6948 - val_accuracy: 0.4058 - lr: 1.0000e-03\nEpoch 3/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.6995 - accuracy: 0.5016\nEpoch 3: val_accuracy did not improve from 0.59420\n21/21 [==============================] - 2s 77ms/step - loss: 0.6995 - accuracy: 0.5016 - val_loss: 0.6585 - val_accuracy: 0.5942 - lr: 1.0000e-03\nEpoch 4/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.6598 - accuracy: 0.5953\nEpoch 4: val_accuracy improved from 0.59420 to 0.67754, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 81ms/step - loss: 0.6608 - accuracy: 0.5935 - val_loss: 0.6362 - val_accuracy: 0.6775 - lr: 1.0000e-03\nEpoch 5/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.6202 - accuracy: 0.6641\nEpoch 5: val_accuracy improved from 0.67754 to 0.73551, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 77ms/step - loss: 0.6190 - accuracy: 0.6651 - val_loss: 0.5421 - val_accuracy: 0.7355 - lr: 1.0000e-03\nEpoch 6/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.5445 - accuracy: 0.7562\nEpoch 6: val_accuracy improved from 0.73551 to 0.74275, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 80ms/step - loss: 0.5441 - accuracy: 0.7570 - val_loss: 0.5304 - val_accuracy: 0.7428 - lr: 1.0000e-03\nEpoch 7/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.4377 - accuracy: 0.8094\nEpoch 7: val_accuracy improved from 0.74275 to 0.82609, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 80ms/step - loss: 0.4374 - accuracy: 0.8100 - val_loss: 0.3783 - val_accuracy: 0.8261 - lr: 1.0000e-03\nEpoch 8/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.4519 - accuracy: 0.8094\nEpoch 8: val_accuracy improved from 0.82609 to 0.82971, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 79ms/step - loss: 0.4510 - accuracy: 0.8100 - val_loss: 0.3805 - val_accuracy: 0.8297 - lr: 1.0000e-03\nEpoch 9/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.4388 - accuracy: 0.8156\nEpoch 9: val_accuracy did not improve from 0.82971\n21/21 [==============================] - 2s 80ms/step - loss: 0.4375 - accuracy: 0.8162 - val_loss: 0.5317 - val_accuracy: 0.7609 - lr: 1.0000e-03\nEpoch 10/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3968 - accuracy: 0.8328\nEpoch 10: val_accuracy improved from 0.82971 to 0.84058, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 81ms/step - loss: 0.3971 - accuracy: 0.8318 - val_loss: 0.3410 - val_accuracy: 0.8406 - lr: 1.0000e-03\nEpoch 11/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3927 - accuracy: 0.8453\nEpoch 11: val_accuracy did not improve from 0.84058\n21/21 [==============================] - 2s 77ms/step - loss: 0.3917 - accuracy: 0.8458 - val_loss: 0.3554 - val_accuracy: 0.8370 - lr: 1.0000e-03\nEpoch 12/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3893 - accuracy: 0.8500\nEpoch 12: val_accuracy improved from 0.84058 to 0.85145, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 79ms/step - loss: 0.3883 - accuracy: 0.8505 - val_loss: 0.3116 - val_accuracy: 0.8514 - lr: 1.0000e-03\nEpoch 13/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3690 - accuracy: 0.8469\nEpoch 13: val_accuracy did not improve from 0.85145\n21/21 [==============================] - 2s 78ms/step - loss: 0.3720 - accuracy: 0.8458 - val_loss: 0.3811 - val_accuracy: 0.8442 - lr: 1.0000e-03\nEpoch 14/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.4070 - accuracy: 0.8422\nEpoch 14: val_accuracy did not improve from 0.85145\n21/21 [==============================] - 2s 77ms/step - loss: 0.4108 - accuracy: 0.8411 - val_loss: 0.3616 - val_accuracy: 0.8406 - lr: 1.0000e-03\nEpoch 15/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.4026 - accuracy: 0.8438\nEpoch 15: val_accuracy improved from 0.85145 to 0.86232, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 79ms/step - loss: 0.4015 - accuracy: 0.8442 - val_loss: 0.3267 - val_accuracy: 0.8623 - lr: 1.0000e-03\nEpoch 16/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3775 - accuracy: 0.8500\nEpoch 16: val_accuracy did not improve from 0.86232\n21/21 [==============================] - 2s 80ms/step - loss: 0.3765 - accuracy: 0.8505 - val_loss: 0.3611 - val_accuracy: 0.8406 - lr: 1.0000e-03\nEpoch 17/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3732 - accuracy: 0.8547\nEpoch 17: val_accuracy did not improve from 0.86232\n21/21 [==============================] - 2s 83ms/step - loss: 0.3778 - accuracy: 0.8536 - val_loss: 0.3102 - val_accuracy: 0.8623 - lr: 1.0000e-03\nEpoch 18/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3794 - accuracy: 0.8625\nEpoch 18: val_accuracy did not improve from 0.86232\n21/21 [==============================] - 2s 89ms/step - loss: 0.3801 - accuracy: 0.8614 - val_loss: 0.3160 - val_accuracy: 0.8514 - lr: 1.0000e-03\nEpoch 19/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3922 - accuracy: 0.8469\nEpoch 19: val_accuracy did not improve from 0.86232\n21/21 [==============================] - 2s 78ms/step - loss: 0.3942 - accuracy: 0.8458 - val_loss: 0.3844 - val_accuracy: 0.8587 - lr: 1.0000e-03\nEpoch 20/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.4093 - accuracy: 0.8406\nEpoch 20: val_accuracy improved from 0.86232 to 0.86594, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 78ms/step - loss: 0.4088 - accuracy: 0.8411 - val_loss: 0.3183 - val_accuracy: 0.8659 - lr: 1.0000e-03\nEpoch 21/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3754 - accuracy: 0.8391\nEpoch 21: val_accuracy did not improve from 0.86594\n21/21 [==============================] - 2s 78ms/step - loss: 0.3757 - accuracy: 0.8380 - val_loss: 0.3132 - val_accuracy: 0.8623 - lr: 1.0000e-03\nEpoch 22/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3625 - accuracy: 0.8516\nEpoch 22: val_accuracy improved from 0.86594 to 0.86957, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 85ms/step - loss: 0.3617 - accuracy: 0.8520 - val_loss: 0.3118 - val_accuracy: 0.8696 - lr: 1.0000e-03\nEpoch 23/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3557 - accuracy: 0.8547\nEpoch 23: val_accuracy did not improve from 0.86957\n21/21 [==============================] - 2s 77ms/step - loss: 0.3553 - accuracy: 0.8551 - val_loss: 0.3057 - val_accuracy: 0.8659 - lr: 5.0000e-04\nEpoch 24/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3606 - accuracy: 0.8578\nEpoch 24: val_accuracy improved from 0.86957 to 0.87319, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 79ms/step - loss: 0.3607 - accuracy: 0.8583 - val_loss: 0.3037 - val_accuracy: 0.8732 - lr: 5.0000e-04\nEpoch 25/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3654 - accuracy: 0.8578\nEpoch 25: val_accuracy did not improve from 0.87319\n21/21 [==============================] - 2s 77ms/step - loss: 0.3650 - accuracy: 0.8583 - val_loss: 0.3308 - val_accuracy: 0.8659 - lr: 5.0000e-04\nEpoch 26/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3621 - accuracy: 0.8531\nEpoch 26: val_accuracy did not improve from 0.87319\n21/21 [==============================] - 2s 75ms/step - loss: 0.3619 - accuracy: 0.8536 - val_loss: 0.3102 - val_accuracy: 0.8587 - lr: 5.0000e-04\nEpoch 27/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3991 - accuracy: 0.8438\nEpoch 27: val_accuracy did not improve from 0.87319\n21/21 [==============================] - 2s 78ms/step - loss: 0.3988 - accuracy: 0.8442 - val_loss: 0.3143 - val_accuracy: 0.8623 - lr: 5.0000e-04\nEpoch 28/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3656 - accuracy: 0.8578\nEpoch 28: val_accuracy did not improve from 0.87319\n21/21 [==============================] - 2s 81ms/step - loss: 0.3687 - accuracy: 0.8551 - val_loss: 0.3335 - val_accuracy: 0.8587 - lr: 5.0000e-04\nEpoch 29/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3628 - accuracy: 0.8578\nEpoch 29: val_accuracy did not improve from 0.87319\n21/21 [==============================] - 2s 78ms/step - loss: 0.3651 - accuracy: 0.8567 - val_loss: 0.3145 - val_accuracy: 0.8623 - lr: 5.0000e-04\nEpoch 30/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3702 - accuracy: 0.8500\nEpoch 30: val_accuracy did not improve from 0.87319\n21/21 [==============================] - 2s 78ms/step - loss: 0.3693 - accuracy: 0.8505 - val_loss: 0.3130 - val_accuracy: 0.8659 - lr: 2.5000e-04\nEpoch 31/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3545 - accuracy: 0.8531\nEpoch 31: val_accuracy did not improve from 0.87319\n21/21 [==============================] - 2s 78ms/step - loss: 0.3549 - accuracy: 0.8536 - val_loss: 0.3232 - val_accuracy: 0.8623 - lr: 2.5000e-04\nEpoch 32/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3531 - accuracy: 0.8562\nEpoch 32: val_accuracy improved from 0.87319 to 0.87681, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 80ms/step - loss: 0.3583 - accuracy: 0.8536 - val_loss: 0.2996 - val_accuracy: 0.8768 - lr: 2.5000e-04\nEpoch 33/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3558 - accuracy: 0.8625\nEpoch 33: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 78ms/step - loss: 0.3558 - accuracy: 0.8629 - val_loss: 0.3148 - val_accuracy: 0.8623 - lr: 2.5000e-04\nEpoch 34/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3584 - accuracy: 0.8547\nEpoch 34: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 81ms/step - loss: 0.3575 - accuracy: 0.8551 - val_loss: 0.2991 - val_accuracy: 0.8732 - lr: 2.5000e-04\nEpoch 35/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3608 - accuracy: 0.8578\nEpoch 35: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 76ms/step - loss: 0.3603 - accuracy: 0.8583 - val_loss: 0.3001 - val_accuracy: 0.8696 - lr: 2.5000e-04\nEpoch 36/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3455 - accuracy: 0.8609\nEpoch 36: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 77ms/step - loss: 0.3446 - accuracy: 0.8614 - val_loss: 0.3059 - val_accuracy: 0.8587 - lr: 2.5000e-04\nEpoch 37/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3528 - accuracy: 0.8578\nEpoch 37: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 95ms/step - loss: 0.3521 - accuracy: 0.8583 - val_loss: 0.3093 - val_accuracy: 0.8551 - lr: 2.5000e-04\nEpoch 38/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3592 - accuracy: 0.8703\nEpoch 38: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 77ms/step - loss: 0.3582 - accuracy: 0.8707 - val_loss: 0.2970 - val_accuracy: 0.8696 - lr: 2.5000e-04\nEpoch 39/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3475 - accuracy: 0.8703\nEpoch 39: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 78ms/step - loss: 0.3468 - accuracy: 0.8707 - val_loss: 0.3067 - val_accuracy: 0.8587 - lr: 2.5000e-04\nEpoch 40/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3472 - accuracy: 0.8578\nEpoch 40: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 80ms/step - loss: 0.3474 - accuracy: 0.8567 - val_loss: 0.3181 - val_accuracy: 0.8551 - lr: 2.5000e-04\nEpoch 41/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3634 - accuracy: 0.8656\nEpoch 41: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 76ms/step - loss: 0.3633 - accuracy: 0.8660 - val_loss: 0.3001 - val_accuracy: 0.8732 - lr: 2.5000e-04\nEpoch 42/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3472 - accuracy: 0.8625\nEpoch 42: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 78ms/step - loss: 0.3466 - accuracy: 0.8629 - val_loss: 0.3067 - val_accuracy: 0.8514 - lr: 2.5000e-04\nEpoch 43/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3480 - accuracy: 0.8531\nEpoch 43: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 76ms/step - loss: 0.3504 - accuracy: 0.8520 - val_loss: 0.2935 - val_accuracy: 0.8732 - lr: 2.5000e-04\nEpoch 44/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3551 - accuracy: 0.8547\nEpoch 44: val_accuracy did not improve from 0.87681\n21/21 [==============================] - 2s 76ms/step - loss: 0.3546 - accuracy: 0.8551 - val_loss: 0.3469 - val_accuracy: 0.8514 - lr: 2.5000e-04\nEpoch 45/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3522 - accuracy: 0.8578\nEpoch 45: val_accuracy improved from 0.87681 to 0.88043, saving model to bilstmkan_best.hdf5\n21/21 [==============================] - 2s 78ms/step - loss: 0.3518 - accuracy: 0.8583 - val_loss: 0.2909 - val_accuracy: 0.8804 - lr: 2.5000e-04\nEpoch 46/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3524 - accuracy: 0.8609\nEpoch 46: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 79ms/step - loss: 0.3513 - accuracy: 0.8614 - val_loss: 0.3171 - val_accuracy: 0.8587 - lr: 2.5000e-04\nEpoch 47/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3454 - accuracy: 0.8625\nEpoch 47: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 78ms/step - loss: 0.3444 - accuracy: 0.8629 - val_loss: 0.3130 - val_accuracy: 0.8587 - lr: 2.5000e-04\nEpoch 48/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3414 - accuracy: 0.8672\nEpoch 48: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 79ms/step - loss: 0.3434 - accuracy: 0.8660 - val_loss: 0.3006 - val_accuracy: 0.8659 - lr: 2.5000e-04\nEpoch 49/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3432 - accuracy: 0.8578\nEpoch 49: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 77ms/step - loss: 0.3424 - accuracy: 0.8583 - val_loss: 0.2985 - val_accuracy: 0.8732 - lr: 2.5000e-04\nEpoch 50/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3579 - accuracy: 0.8594\nEpoch 50: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 79ms/step - loss: 0.3573 - accuracy: 0.8598 - val_loss: 0.2940 - val_accuracy: 0.8804 - lr: 2.5000e-04\nEpoch 51/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3451 - accuracy: 0.8656\nEpoch 51: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 77ms/step - loss: 0.3443 - accuracy: 0.8660 - val_loss: 0.3014 - val_accuracy: 0.8623 - lr: 1.2500e-04\nEpoch 52/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3459 - accuracy: 0.8688\nEpoch 52: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 77ms/step - loss: 0.3466 - accuracy: 0.8676 - val_loss: 0.3056 - val_accuracy: 0.8587 - lr: 1.2500e-04\nEpoch 53/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3393 - accuracy: 0.8641\nEpoch 53: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 77ms/step - loss: 0.3397 - accuracy: 0.8629 - val_loss: 0.3022 - val_accuracy: 0.8659 - lr: 1.2500e-04\nEpoch 54/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3503 - accuracy: 0.8609\nEpoch 54: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 76ms/step - loss: 0.3500 - accuracy: 0.8614 - val_loss: 0.2916 - val_accuracy: 0.8696 - lr: 1.2500e-04\nEpoch 55/100\n20/21 [===========================>..] - ETA: 0s - loss: 0.3476 - accuracy: 0.8531\nEpoch 55: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 2s 77ms/step - loss: 0.3482 - accuracy: 0.8520 - val_loss: 0.3035 - val_accuracy: 0.8587 - lr: 1.2500e-04\n","output_type":"stream"}]},{"cell_type":"code","source":"best_model = model\nbest_model.evaluate(testX, testy)\nbest_model.evaluate(trainX, trainy)\npredy = best_model.predict(testX)\n\n# train_labels_pred= (predy = model.predict(trainX) > 0.5).astype(int).flatten()\ntrain_labels_real=trainy\nrounded_labels_pred= (predy > 0.5).astype(int).flatten()\nrounded_labels_real=testy\n\nlabels = ['Normal','Heart Disease']\nfrom sklearn.metrics import classification_report, roc_auc_score\n# print(classification_report(rounded_labels_real, rounded_labels_pred, target_names=labels))\nprint(classification_report(testy, rounded_labels_pred, target_names=labels))\n\nfrom sklearn.metrics import classification_report, matthews_corrcoef, cohen_kappa_score\n\n# Calculate MCC and Kappa scores\nmcc = matthews_corrcoef(testy, rounded_labels_pred)\nkappa = cohen_kappa_score(testy, rounded_labels_pred)\n\n# Print MCC and Kappa scores\nprint(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\nprint(f\"Cohen's Kappa Score: {kappa}\")\nprint(\"roc: \", roc_auc_score(rounded_labels_real, predy))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T20:49:41.390863Z","iopub.execute_input":"2024-08-13T20:49:41.391238Z","iopub.status.idle":"2024-08-13T20:49:43.569986Z","shell.execute_reply.started":"2024-08-13T20:49:41.391207Z","shell.execute_reply":"2024-08-13T20:49:43.568980Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"9/9 [==============================] - 0s 24ms/step - loss: 0.2909 - accuracy: 0.8804\n21/21 [==============================] - 1s 24ms/step - loss: 0.3429 - accuracy: 0.8645\n9/9 [==============================] - 1s 22ms/step\n               precision    recall  f1-score   support\n\n       Normal       0.86      0.84      0.85       112\nHeart Disease       0.89      0.91      0.90       164\n\n     accuracy                           0.88       276\n    macro avg       0.88      0.87      0.88       276\n weighted avg       0.88      0.88      0.88       276\n\nMatthews Correlation Coefficient (MCC): 0.7512039489056183\nCohen's Kappa Score: 0.7510114816839804\nroc:  0.9503484320557491\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model-2: QCKANnet","metadata":{}},{"cell_type":"code","source":"from tfkan.layers import DenseKAN, Conv2DKAN, Conv1DKAN\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import models, layers, optimizers, metrics\nimport pennylane as qml\nimport tensorflow as tf\nfrom silence_tensorflow import silence_tensorflow\nsilence_tensorflow()\ntf.keras.backend.set_floatx('float64')\ntf.keras.backend.clear_session()\nfrom pennylane import numpy as np\n\n### essentials ###\nn_qubits = 4\nlayers = 4\ndata_dimension = 1  #len(YCOLS) #Y.shape[1]  ## output dimenstion according to one-hot encoding depth\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n## extras ###\nn_block_wires = 2\nn_params_block = 2\nn_blocks = qml.TTN.get_n_blocks(range(n_qubits), n_block_wires)\n\n@qml.qnode(dev)\ndef qnode(weights, inputs=None):\n    qml.templates.AmplitudeEmbedding(features=inputs, wires=range(n_qubits),normalize=True, pad_with=True) # first turn it true, then false, again true. It'll work\n    #qml.templates.AngleEmbedding(features=inputs, wires=range(n_qubits), rotation='Z')\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nweight_shapes = {\"weights\": (layers, n_qubits, 3)}\n\nqlayer_1 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits, name='QuantumLayer1')\nqlayer_2 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits, name='QuantumLayer2')\nqlayer_3 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits, name='QuantumLayer3')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:15:10.199464Z","iopub.execute_input":"2024-08-13T22:15:10.199843Z","iopub.status.idle":"2024-08-13T22:15:10.236248Z","shell.execute_reply.started":"2024-08-13T22:15:10.199814Z","shell.execute_reply":"2024-08-13T22:15:10.235001Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom kerastuner import HyperModel\nfrom kerastuner.tuners import Hyperband\nfrom kerastuner.engine.hyperparameters import HyperParameters\n\nbatch_size = 10\n# Define the input layer\ninputs = layers.Input(shape=(trainX_reshaped.shape[1], 1))\n\nclass MyHyperModel(HyperModel):\n    def build(self, hp):\n        model4 = Conv1DKAN(filters=hp.Int('filters_1', min_value=32, max_value=128, step=32), kernel_size=3, strides=2, padding='valid', kan_kwargs={'grid_size': 3})(inputs)\n        model4 = Conv1DKAN(filters=hp.Int('filters_1', min_value=32, max_value=128, step=32), kernel_size=2, strides=2, padding='valid', kan_kwargs={'grid_size': 3})(model4)\n        model4 = layers.Dropout(0.2)(model4)\n        model4 = layers.Flatten()(model4)\n        model4= DenseKAN((2**n_qubits)*3)(model4)\n        x_1, x_2, x_3 = tf.split(model4, 3, axis=1)\n        x_1 = qlayer_1(x_1)\n        x_2 = qlayer_2(x_2)\n        x_3 = qlayer_3(x_3)\n        model4 = tf.concat([x_1, x_2, x_3], axis=1)\n        model4 = DenseKAN(256, grid_size=3)(model4)\n        model4 = layers.Dropout(0.2)(model4)\n        model4 = DenseKAN(32, grid_size=3)(model4)\n        model4 = layers.Dropout(0.2)(model4)\n\n        outputs = layers.Dense(data_dimension, activation='sigmoid')(model4)\n\n        model = models.Model(inputs=inputs, outputs=outputs)\n\n        learning_rate = 1e-3\n        optimizer = optimizers.Adam(learning_rate=learning_rate)\n        model.compile(\n            loss='binary_crossentropy',\n            optimizer=optimizer,\n            metrics='accuracy'\n        )\n\n        return model\n\n# Define the hypermodel\nhypermodel = MyHyperModel()\n\n# Initialize the tuner\ntuner = Hyperband(\n    hypermodel,\n    objective='val_accuracy',\n    max_epochs=10,\n    factor=3,\n    directory='my_dir',\n    project_name='ht0'\n)\n\n# Print the results summary\ntuner.results_summary()\n\n# Search for the best hyperparameters\ntuner.search(trainX, trainy,\n             batch_size=batch_size,\n             epochs=50,\n             validation_data=(testX, testy),\n             callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n\n# Get the best hyperparameters and model\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nbest_model = tuner.hypermodel.build(best_hps)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-15T19:58:01.722512Z","iopub.execute_input":"2024-06-15T19:58:01.723822Z","iopub.status.idle":"2024-06-15T20:05:30.632343Z","shell.execute_reply.started":"2024-06-15T19:58:01.723763Z","shell.execute_reply":"2024-06-15T20:05:30.630997Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Trial 4 Complete [00h 01m 53s]\nval_accuracy: 0.8152173913043478\n\nBest val_accuracy So Far: 0.8188405797101449\nTotal elapsed time: 00h 07m 27s\n","output_type":"stream"}]},{"cell_type":"code","source":"tuner.results_summary()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-15T20:10:29.744705Z","iopub.execute_input":"2024-06-15T20:10:29.745228Z","iopub.status.idle":"2024-06-15T20:10:29.753460Z","shell.execute_reply.started":"2024-06-15T20:10:29.745189Z","shell.execute_reply":"2024-06-15T20:10:29.752116Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Results summary\nResults in my_dir/ht0\nShowing 10 best trials\nObjective(name=\"val_accuracy\", direction=\"max\")\n\nTrial 0002 summary\nHyperparameters:\nfilters_1: 96\ntuner/epochs: 2\ntuner/initial_epoch: 0\ntuner/bracket: 2\ntuner/round: 0\nScore: 0.8188405797101449\n\nTrial 0003 summary\nHyperparameters:\nfilters_1: 128\ntuner/epochs: 2\ntuner/initial_epoch: 0\ntuner/bracket: 2\ntuner/round: 0\nScore: 0.8152173913043478\n\nTrial 0000 summary\nHyperparameters:\nfilters_1: 64\ntuner/epochs: 2\ntuner/initial_epoch: 0\ntuner/bracket: 2\ntuner/round: 0\nScore: 0.8079710144927537\n\nTrial 0001 summary\nHyperparameters:\nfilters_1: 32\ntuner/epochs: 2\ntuner/initial_epoch: 0\ntuner/bracket: 2\ntuner/round: 0\nScore: 0.8079710144927537\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout,Lambda, LSTM\nfrom keras.initializers import random_uniform\nbatch_size = 10\n# Define the input layer\ninputs = layers.Input(shape=(trainX_reshaped.shape[1], 1))\n\n\nmodel4 = Conv1DKAN(filters=96, kernel_size=3, strides=2, padding='valid', kan_kwargs={'grid_size': 3})(inputs)\nmodel4 = Conv1DKAN(filters=96, kernel_size=2, strides=2, padding='valid', kan_kwargs={'grid_size': 3})(model4)\nmodel4 = layers.Dropout(0.2)(model4)\nmodel4 = layers.Flatten()(model4)\nmodel4= DenseKAN((2**n_qubits)*3)(model4)\nx_1, x_2, x_3 = tf.split(model4, 3, axis=1)\nx_1 = qlayer_1(x_1)\nx_2 = qlayer_2(x_2)\nx_3 = qlayer_3(x_3)\nmodel4 = tf.concat([x_1, x_2, x_3], axis=1)\nmodel4 = DenseKAN(256, grid_size=3)(model4)\nmodel4 = layers.Dropout(0.2)(model4)\nmodel4 = DenseKAN(32, grid_size=3)(model4)\nmodel4 = layers.Dropout(0.2)(model4)\noutputs = layers.Dense(data_dimension, activation='sigmoid')(model4)\nmodel = models.Model(inputs=inputs, outputs=outputs)\n\n\nclass ReduceLRBacktrack(tf.keras.callbacks.ReduceLROnPlateau):\n    def __init__(self, best_path, *args, **kwargs):\n        super(ReduceLRBacktrack, self).__init__(*args, **kwargs)\n        self.best_path = best_path\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(self.monitor)\n        if current is None:\n            logging.warning('Reduce LR on plateau conditioned on metric `%s` '\n                            'which is not available. Available metrics are: %s',\n                             self.monitor, ','.join(list(logs.keys())))\n        if not self.monitor_op(current, self.best): # not new best\n            if not self.in_cooldown(): # and we're not in cooldown\n                if self.wait+1 >= self.patience: # going to reduce lr\n                    # load best model so far\n                    # print(\"\\n Backtracking to best model before reducting LR\")\n                    self.model.load_weights(self.best_path)\n\n        super().on_epoch_end(epoch, logs) # actually reduce LR\n\nmodel_path = 'qcnn_4qubit_4layer.hdf5' # hdf5 for TF v2 2.14.0, for others h5 or keras or tf\n\nlr_reducer = ReduceLRBacktrack(\n    best_path = model_path,\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=5,\n    mode='min',\n    min_lr=1e-10\n)\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    model_path, save_weights_only=True, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1\n)\n\n# Compile the model\nlearning_rate = 1e-3\noptimizer = optimizers.Adam(learning_rate=learning_rate)\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\nfrom tensorflow.keras.metrics import BinaryAccuracy\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=optimizer,\n    metrics='accuracy'\n)\nhistory = model.fit(\n    trainX,\n    trainy,\n    # batch_size=batch_size,\n    epochs = 100,\n    callbacks=[checkpoint, lr_reducer, early_stopping],\n    validation_data = [testX, testy]\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-13T22:15:20.105061Z","iopub.execute_input":"2024-08-13T22:15:20.105477Z","iopub.status.idle":"2024-08-13T22:31:51.296819Z","shell.execute_reply.started":"2024-08-13T22:15:20.105445Z","shell.execute_reply":"2024-08-13T22:31:51.295815Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch 1/100\n21/21 [==============================] - ETA: 0s - loss: 0.6570 - accuracy: 0.6184\nEpoch 1: val_accuracy improved from -inf to 0.78623, saving model to qcnn_4qubit_4layer.hdf5\n21/21 [==============================] - 33s 2s/step - loss: 0.6570 - accuracy: 0.6184 - val_loss: 0.5391 - val_accuracy: 0.7862 - lr: 1.0000e-03\nEpoch 2/100\n21/21 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.7710\nEpoch 2: val_accuracy did not improve from 0.78623\n21/21 [==============================] - 32s 2s/step - loss: 0.5120 - accuracy: 0.7710 - val_loss: 0.5084 - val_accuracy: 0.7681 - lr: 1.0000e-03\nEpoch 3/100\n21/21 [==============================] - ETA: 0s - loss: 0.4852 - accuracy: 0.7928\nEpoch 3: val_accuracy did not improve from 0.78623\n21/21 [==============================] - 32s 2s/step - loss: 0.4852 - accuracy: 0.7928 - val_loss: 0.4997 - val_accuracy: 0.7645 - lr: 1.0000e-03\nEpoch 4/100\n21/21 [==============================] - ETA: 0s - loss: 0.4730 - accuracy: 0.7804\nEpoch 4: val_accuracy improved from 0.78623 to 0.79710, saving model to qcnn_4qubit_4layer.hdf5\n21/21 [==============================] - 32s 2s/step - loss: 0.4730 - accuracy: 0.7804 - val_loss: 0.4309 - val_accuracy: 0.7971 - lr: 1.0000e-03\nEpoch 5/100\n21/21 [==============================] - ETA: 0s - loss: 0.4824 - accuracy: 0.7850\nEpoch 5: val_accuracy improved from 0.79710 to 0.80435, saving model to qcnn_4qubit_4layer.hdf5\n21/21 [==============================] - 32s 2s/step - loss: 0.4824 - accuracy: 0.7850 - val_loss: 0.4185 - val_accuracy: 0.8043 - lr: 1.0000e-03\nEpoch 6/100\n21/21 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.7819\nEpoch 6: val_accuracy did not improve from 0.80435\n21/21 [==============================] - 32s 2s/step - loss: 0.4690 - accuracy: 0.7819 - val_loss: 0.4466 - val_accuracy: 0.8043 - lr: 1.0000e-03\nEpoch 7/100\n21/21 [==============================] - ETA: 0s - loss: 0.4738 - accuracy: 0.8022\nEpoch 7: val_accuracy did not improve from 0.80435\n21/21 [==============================] - 32s 2s/step - loss: 0.4738 - accuracy: 0.8022 - val_loss: 0.4207 - val_accuracy: 0.7899 - lr: 1.0000e-03\nEpoch 8/100\n21/21 [==============================] - ETA: 0s - loss: 0.5034 - accuracy: 0.7726\nEpoch 8: val_accuracy did not improve from 0.80435\n21/21 [==============================] - 32s 2s/step - loss: 0.5034 - accuracy: 0.7726 - val_loss: 0.4809 - val_accuracy: 0.7609 - lr: 1.0000e-03\nEpoch 9/100\n21/21 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.7773\nEpoch 9: val_accuracy improved from 0.80435 to 0.81159, saving model to qcnn_4qubit_4layer.hdf5\n21/21 [==============================] - 32s 2s/step - loss: 0.4801 - accuracy: 0.7773 - val_loss: 0.4309 - val_accuracy: 0.8116 - lr: 1.0000e-03\nEpoch 10/100\n21/21 [==============================] - ETA: 0s - loss: 0.4693 - accuracy: 0.7913\nEpoch 10: val_accuracy did not improve from 0.81159\n21/21 [==============================] - 32s 2s/step - loss: 0.4693 - accuracy: 0.7913 - val_loss: 0.4205 - val_accuracy: 0.8080 - lr: 1.0000e-03\nEpoch 11/100\n21/21 [==============================] - ETA: 0s - loss: 0.4452 - accuracy: 0.7944\nEpoch 11: val_accuracy did not improve from 0.81159\n21/21 [==============================] - 32s 2s/step - loss: 0.4452 - accuracy: 0.7944 - val_loss: 0.4229 - val_accuracy: 0.8080 - lr: 5.0000e-04\nEpoch 12/100\n21/21 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.7882\nEpoch 12: val_accuracy did not improve from 0.81159\n21/21 [==============================] - 32s 2s/step - loss: 0.4477 - accuracy: 0.7882 - val_loss: 0.4211 - val_accuracy: 0.8116 - lr: 5.0000e-04\nEpoch 13/100\n21/21 [==============================] - ETA: 0s - loss: 0.4354 - accuracy: 0.8100\nEpoch 13: val_accuracy did not improve from 0.81159\n21/21 [==============================] - 32s 2s/step - loss: 0.4354 - accuracy: 0.8100 - val_loss: 0.4055 - val_accuracy: 0.8080 - lr: 5.0000e-04\nEpoch 14/100\n21/21 [==============================] - ETA: 0s - loss: 0.4424 - accuracy: 0.8053\nEpoch 14: val_accuracy did not improve from 0.81159\n21/21 [==============================] - 32s 2s/step - loss: 0.4424 - accuracy: 0.8053 - val_loss: 0.4044 - val_accuracy: 0.8116 - lr: 5.0000e-04\nEpoch 15/100\n21/21 [==============================] - ETA: 0s - loss: 0.4404 - accuracy: 0.8053\nEpoch 15: val_accuracy improved from 0.81159 to 0.81884, saving model to qcnn_4qubit_4layer.hdf5\n21/21 [==============================] - 32s 2s/step - loss: 0.4404 - accuracy: 0.8053 - val_loss: 0.3962 - val_accuracy: 0.8188 - lr: 5.0000e-04\nEpoch 16/100\n21/21 [==============================] - ETA: 0s - loss: 0.4394 - accuracy: 0.8006\nEpoch 16: val_accuracy did not improve from 0.81884\n21/21 [==============================] - 32s 2s/step - loss: 0.4394 - accuracy: 0.8006 - val_loss: 0.3990 - val_accuracy: 0.8152 - lr: 5.0000e-04\nEpoch 17/100\n21/21 [==============================] - ETA: 0s - loss: 0.4374 - accuracy: 0.8006\nEpoch 17: val_accuracy improved from 0.81884 to 0.82971, saving model to qcnn_4qubit_4layer.hdf5\n21/21 [==============================] - 32s 2s/step - loss: 0.4374 - accuracy: 0.8006 - val_loss: 0.3923 - val_accuracy: 0.8297 - lr: 5.0000e-04\nEpoch 18/100\n21/21 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.8100\nEpoch 18: val_accuracy did not improve from 0.82971\n21/21 [==============================] - 32s 2s/step - loss: 0.4243 - accuracy: 0.8100 - val_loss: 0.3992 - val_accuracy: 0.8080 - lr: 5.0000e-04\nEpoch 19/100\n21/21 [==============================] - ETA: 0s - loss: 0.4402 - accuracy: 0.8084\nEpoch 19: val_accuracy did not improve from 0.82971\n21/21 [==============================] - 32s 2s/step - loss: 0.4402 - accuracy: 0.8084 - val_loss: 0.4183 - val_accuracy: 0.8116 - lr: 5.0000e-04\nEpoch 20/100\n21/21 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.8146\nEpoch 20: val_accuracy did not improve from 0.82971\n21/21 [==============================] - 32s 2s/step - loss: 0.4299 - accuracy: 0.8146 - val_loss: 0.4025 - val_accuracy: 0.8116 - lr: 5.0000e-04\nEpoch 21/100\n21/21 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.7960\nEpoch 21: val_accuracy improved from 0.82971 to 0.83333, saving model to qcnn_4qubit_4layer.hdf5\n21/21 [==============================] - 32s 2s/step - loss: 0.4281 - accuracy: 0.7960 - val_loss: 0.3850 - val_accuracy: 0.8333 - lr: 5.0000e-04\nEpoch 22/100\n21/21 [==============================] - ETA: 0s - loss: 0.4338 - accuracy: 0.8131\nEpoch 22: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 32s 2s/step - loss: 0.4338 - accuracy: 0.8131 - val_loss: 0.4134 - val_accuracy: 0.8225 - lr: 5.0000e-04\nEpoch 23/100\n21/21 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.8100\nEpoch 23: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 32s 2s/step - loss: 0.4387 - accuracy: 0.8100 - val_loss: 0.4207 - val_accuracy: 0.8080 - lr: 5.0000e-04\nEpoch 24/100\n21/21 [==============================] - ETA: 0s - loss: 0.4327 - accuracy: 0.8302\nEpoch 24: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 32s 2s/step - loss: 0.4327 - accuracy: 0.8302 - val_loss: 0.3863 - val_accuracy: 0.8261 - lr: 5.0000e-04\nEpoch 25/100\n21/21 [==============================] - ETA: 0s - loss: 0.4188 - accuracy: 0.8131\nEpoch 25: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 32s 2s/step - loss: 0.4188 - accuracy: 0.8131 - val_loss: 0.3892 - val_accuracy: 0.8225 - lr: 5.0000e-04\nEpoch 26/100\n21/21 [==============================] - ETA: 0s - loss: 0.4239 - accuracy: 0.8178\nEpoch 26: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 32s 2s/step - loss: 0.4239 - accuracy: 0.8178 - val_loss: 0.4064 - val_accuracy: 0.8043 - lr: 5.0000e-04\nEpoch 27/100\n21/21 [==============================] - ETA: 0s - loss: 0.4229 - accuracy: 0.8209\nEpoch 27: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 31s 2s/step - loss: 0.4229 - accuracy: 0.8209 - val_loss: 0.3872 - val_accuracy: 0.8188 - lr: 2.5000e-04\nEpoch 28/100\n21/21 [==============================] - ETA: 0s - loss: 0.4115 - accuracy: 0.8209\nEpoch 28: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 32s 2s/step - loss: 0.4115 - accuracy: 0.8209 - val_loss: 0.3866 - val_accuracy: 0.8080 - lr: 2.5000e-04\nEpoch 29/100\n21/21 [==============================] - ETA: 0s - loss: 0.4142 - accuracy: 0.8162\nEpoch 29: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 32s 2s/step - loss: 0.4142 - accuracy: 0.8162 - val_loss: 0.3873 - val_accuracy: 0.8116 - lr: 2.5000e-04\nEpoch 30/100\n21/21 [==============================] - ETA: 0s - loss: 0.4140 - accuracy: 0.8224\nEpoch 30: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 32s 2s/step - loss: 0.4140 - accuracy: 0.8224 - val_loss: 0.3885 - val_accuracy: 0.8116 - lr: 2.5000e-04\nEpoch 31/100\n21/21 [==============================] - ETA: 0s - loss: 0.4121 - accuracy: 0.8209\nEpoch 31: val_accuracy did not improve from 0.83333\n21/21 [==============================] - 32s 2s/step - loss: 0.4121 - accuracy: 0.8209 - val_loss: 0.4126 - val_accuracy: 0.8152 - lr: 2.5000e-04\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model 3: QDenseKANnet","metadata":{}},{"cell_type":"code","source":"from tfkan import layers\nfrom tfkan.layers import DenseKAN, Conv2DKAN, Conv1DKAN\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import models, layers, optimizers, metrics\nimport pennylane as qml\nimport tensorflow as tf\nfrom silence_tensorflow import silence_tensorflow\nsilence_tensorflow()\ntf.keras.backend.set_floatx('float64')\ntf.keras.backend.clear_session()\nfrom pennylane import numpy as np\n\n### essentials ###\nn_qubits = 4\nlayers = 1\ndata_dimension = 1  #len(YCOLS) #Y.shape[1]  ## output dimenstion according to one-hot encoding depth\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n## extras ###\nn_block_wires = 2\nn_params_block = 2\nn_blocks = qml.TTN.get_n_blocks(range(n_qubits), n_block_wires)\n\n@qml.qnode(dev)\ndef qnode(weights, inputs=None):\n    qml.templates.AmplitudeEmbedding(features=inputs, wires=range(n_qubits),normalize=True, pad_with=True) # first turn it true, then false, again true. It'll work\n    #qml.templates.AngleEmbedding(features=inputs, wires=range(n_qubits), rotation='Z')\n    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n\nweight_shapes = {\"weights\": (layers, n_qubits, 3)}\n\nqlayer_1 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits, name='QuantumLayer1')\nqlayer_2 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits, name='QuantumLayer2')\nqlayer_3 = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=n_qubits, name='QuantumLayer3')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T20:26:45.993282Z","iopub.execute_input":"2024-06-15T20:26:45.993805Z","iopub.status.idle":"2024-06-15T20:26:46.086126Z","shell.execute_reply.started":"2024-06-15T20:26:45.993767Z","shell.execute_reply":"2024-06-15T20:26:46.084867Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom kerastuner import HyperModel\nfrom kerastuner.tuners import Hyperband\nfrom kerastuner.engine.hyperparameters import HyperParameters\n\nbatch_size = 10\n# Define the input layer\ninputs = layers.Input(shape=(trainX_reshaped.shape[1], 1))\n\nclass MyHyperModel(HyperModel):\n    def build(self, hp):\n        model4 = DenseKAN(units=hp.Int('units_1', min_value=16, max_value=256, step=32), grid_size=3)(inputs)\n        model4 = layers.Flatten()(model4)\n        model4 = layers.Dropout(0.2)(model4)\n        model4 = DenseKAN(units=hp.Int('units_2', min_value=16, max_value=256, step=32), grid_size=3)(model4)\n        model4 = layers.Dropout(0.2)(model4)\n        model4= DenseKAN((2**n_qubits)*3)(model4)\n        x_1, x_2, x_3 = tf.split(model4, 3, axis=1)\n        x_1 = qlayer_1(x_1)\n        x_2 = qlayer_2(x_2)\n        x_3 = qlayer_3(x_3)\n        model4 = tf.concat([x_1, x_2, x_3], axis=1)\n        model4 = DenseKAN(units=hp.Int('units_3', min_value=16, max_value=256, step=32), grid_size=3)(model4)\n        model4 = layers.Dropout(0.2)(model4)\n        model4 = DenseKAN(32, grid_size=3)(model4)\n        model4 = layers.Dropout(0.2)(model4)\n\n        outputs = layers.Dense(data_dimension, activation='sigmoid')(model4)\n\n        model = models.Model(inputs=inputs, outputs=outputs)\n\n        learning_rate = 1e-3\n        optimizer = optimizers.Adam(learning_rate=learning_rate)\n        model.compile(\n            loss='binary_crossentropy',\n            optimizer=optimizer,\n            metrics='accuracy'\n        )\n\n        return model\n\n# Define the hypermodel\nhypermodel = MyHyperModel()\n\n# Initialize the tuner\ntuner = Hyperband(\n    hypermodel,\n    objective='val_accuracy',\n    max_epochs=10,\n    factor=3,\n    directory='my_dir',\n    project_name='ht1'\n)\n\n# Print the results summary\ntuner.results_summary()\n\n# Search for the best hyperparameters\ntuner.search(trainX, trainy,\n             batch_size=batch_size,\n             epochs=50,\n             validation_data=(testX, testy),\n             callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n\n# Get the best hyperparameters and model\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nbest_model = tuner.hypermodel.build(best_hps)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-15T20:27:08.196580Z","iopub.execute_input":"2024-06-15T20:27:08.196979Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Trial 26 Complete [00h 03m 32s]\nval_accuracy: 0.8695652173913043\n\nBest val_accuracy So Far: 0.8840579710144928\nTotal elapsed time: 01h 12m 44s\n\nSearch: Running Trial #27\n\nValue             |Best Value So Far |Hyperparameter\n208               |112               |units_1\n16                |176               |units_2\n144               |112               |units_3\n10                |10                |tuner/epochs\n0                 |4                 |tuner/initial_epoch\n0                 |2                 |tuner/bracket\n0                 |2                 |tuner/round\n\nEpoch 1/10\n65/65 [==============================] - 55s 846ms/step - loss: 0.7053 - accuracy: 0.5421 - val_loss: 0.6827 - val_accuracy: 0.6957\nEpoch 2/10\n65/65 [==============================] - 53s 817ms/step - loss: 0.6143 - accuracy: 0.6558 - val_loss: 0.5216 - val_accuracy: 0.7754\nEpoch 3/10\n65/65 [==============================] - 53s 820ms/step - loss: 0.4618 - accuracy: 0.8162 - val_loss: 0.5658 - val_accuracy: 0.7572\nEpoch 4/10\n65/65 [==============================] - 53s 816ms/step - loss: 0.4351 - accuracy: 0.8271 - val_loss: 0.3589 - val_accuracy: 0.8696\nEpoch 5/10\n65/65 [==============================] - 54s 833ms/step - loss: 0.4131 - accuracy: 0.8474 - val_loss: 0.3589 - val_accuracy: 0.8804\nEpoch 6/10\n65/65 [==============================] - 54s 837ms/step - loss: 0.4096 - accuracy: 0.8333 - val_loss: 0.3506 - val_accuracy: 0.8841\nEpoch 7/10\n65/65 [==============================] - 53s 823ms/step - loss: 0.4074 - accuracy: 0.8396 - val_loss: 0.3457 - val_accuracy: 0.8732\nEpoch 8/10\n65/65 [==============================] - 54s 826ms/step - loss: 0.3967 - accuracy: 0.8458 - val_loss: 0.5607 - val_accuracy: 0.7464\nEpoch 9/10\n65/65 [==============================] - 54s 826ms/step - loss: 0.4083 - accuracy: 0.8349 - val_loss: 0.3260 - val_accuracy: 0.8841\nEpoch 10/10\n48/65 [=====================>........] - ETA: 11s - loss: 0.4140 - accuracy: 0.8271","output_type":"stream"}]},{"cell_type":"code","source":"tuner.results_summary()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout,Lambda, LSTM\nfrom keras.initializers import random_uniform\nbatch_size = 10\n# Define the input layer\ninputs = layers.Input(shape=(trainX_reshaped.shape[1], 1))\n\n\nmodel2 = DenseKAN(112, grid_size=3)(inputs)\nmodel2 = layers.Flatten()(model2)\nmodel2 = layers.Dropout(0.2)(model2)\nmodel2 = DenseKAN(176, grid_size=3)(model2)\nmodel2 = layers.Dropout(0.2)(model2)\n# model2= DenseKAN((3*n_qubits))(model2)\nmodel2= DenseKAN((2**n_qubits)*3)(model2)\nx_1, x_2, x_3 = tf.split(model2, 3, axis=1)\nx_1 = qlayer_1(x_1)\nx_2 = qlayer_2(x_2)\nx_3 = qlayer_3(x_3)\nmodel2 = tf.concat([x_1, x_2, x_3], axis=1)\nmodel2 = DenseKAN(112, grid_size=3)(model2)\nmodel2 = layers.Dropout(0.2)(model2)\nmodel2 = DenseKAN(32, grid_size=3)(model2)\nmodel2 = layers.Dropout(0.2)(model2)\noutputs = layers.Dense(data_dimension, activation='sigmoid')(model2)\nmodel = models.Model(inputs=inputs, outputs=outputs)\n\n\nclass ReduceLRBacktrack(tf.keras.callbacks.ReduceLROnPlateau):\n    def __init__(self, best_path, *args, **kwargs):\n        super(ReduceLRBacktrack, self).__init__(*args, **kwargs)\n        self.best_path = best_path\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(self.monitor)\n        if current is None:\n            logging.warning('Reduce LR on plateau conditioned on metric `%s` '\n                            'which is not available. Available metrics are: %s',\n                             self.monitor, ','.join(list(logs.keys())))\n        if not self.monitor_op(current, self.best): # not new best\n            if not self.in_cooldown(): # and we're not in cooldown\n                if self.wait+1 >= self.patience: # going to reduce lr\n                    # load best model so far\n                    # print(\"\\n Backtracking to best model before reducting LR\")\n                    self.model.load_weights(self.best_path)\n\n        super().on_epoch_end(epoch, logs) # actually reduce LR\n\nmodel_path = 'qdense_best.hdf5' # hdf5 for TF v2 2.14.0, for others h5 or keras or tf\n\nlr_reducer = ReduceLRBacktrack(\n    best_path = model_path,\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=5,\n    mode='min',\n    min_lr=1e-10\n)\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    model_path, save_weights_only=True, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1\n)\n\n# Compile the model\nlearning_rate = 1e-3\noptimizer = optimizers.Adam(learning_rate=learning_rate)\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\nfrom tensorflow.keras.metrics import BinaryAccuracy\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=optimizer,\n    metrics='accuracy'\n)\nhistory = model.fit(\n    trainX,\n    trainy,\n    # batch_size=batch_size,\n    epochs = 100,\n    callbacks=[checkpoint, lr_reducer, early_stopping],\n    validation_data = [testX, testy]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T20:32:15.741796Z","iopub.execute_input":"2024-08-13T20:32:15.742690Z","iopub.status.idle":"2024-08-13T20:47:45.998895Z","shell.execute_reply.started":"2024-08-13T20:32:15.742655Z","shell.execute_reply":"2024-08-13T20:47:45.998075Z"},"scrolled":true,"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch 1/100\n21/21 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.5670\nEpoch 1: val_accuracy improved from -inf to 0.80797, saving model to qdense_best.hdf5\n21/21 [==============================] - 23s 1s/step - loss: 0.6840 - accuracy: 0.5670 - val_loss: 0.6677 - val_accuracy: 0.8080 - lr: 1.0000e-03\nEpoch 2/100\n21/21 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.7632\nEpoch 2: val_accuracy improved from 0.80797 to 0.81522, saving model to qdense_best.hdf5\n21/21 [==============================] - 23s 1s/step - loss: 0.5755 - accuracy: 0.7632 - val_loss: 0.4130 - val_accuracy: 0.8152 - lr: 1.0000e-03\nEpoch 3/100\n21/21 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.8380\nEpoch 3: val_accuracy improved from 0.81522 to 0.84058, saving model to qdense_best.hdf5\n21/21 [==============================] - 23s 1s/step - loss: 0.4243 - accuracy: 0.8380 - val_loss: 0.3673 - val_accuracy: 0.8406 - lr: 1.0000e-03\nEpoch 4/100\n21/21 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.8380\nEpoch 4: val_accuracy improved from 0.84058 to 0.85507, saving model to qdense_best.hdf5\n21/21 [==============================] - 24s 1s/step - loss: 0.4133 - accuracy: 0.8380 - val_loss: 0.3536 - val_accuracy: 0.8551 - lr: 1.0000e-03\nEpoch 5/100\n21/21 [==============================] - ETA: 0s - loss: 0.4214 - accuracy: 0.8302\nEpoch 5: val_accuracy did not improve from 0.85507\n21/21 [==============================] - 23s 1s/step - loss: 0.4214 - accuracy: 0.8302 - val_loss: 0.3601 - val_accuracy: 0.8478 - lr: 1.0000e-03\nEpoch 6/100\n21/21 [==============================] - ETA: 0s - loss: 0.3993 - accuracy: 0.8302\nEpoch 6: val_accuracy did not improve from 0.85507\n21/21 [==============================] - 23s 1s/step - loss: 0.3993 - accuracy: 0.8302 - val_loss: 0.3855 - val_accuracy: 0.8478 - lr: 1.0000e-03\nEpoch 7/100\n21/21 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.8318\nEpoch 7: val_accuracy did not improve from 0.85507\n21/21 [==============================] - 23s 1s/step - loss: 0.4262 - accuracy: 0.8318 - val_loss: 0.3954 - val_accuracy: 0.8188 - lr: 1.0000e-03\nEpoch 8/100\n21/21 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.8224\nEpoch 8: val_accuracy did not improve from 0.85507\n21/21 [==============================] - 23s 1s/step - loss: 0.4249 - accuracy: 0.8224 - val_loss: 0.4109 - val_accuracy: 0.8080 - lr: 1.0000e-03\nEpoch 9/100\n21/21 [==============================] - ETA: 0s - loss: 0.4066 - accuracy: 0.8442\nEpoch 9: val_accuracy did not improve from 0.85507\n21/21 [==============================] - 23s 1s/step - loss: 0.4066 - accuracy: 0.8442 - val_loss: 0.3898 - val_accuracy: 0.8297 - lr: 1.0000e-03\nEpoch 10/100\n21/21 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8333\nEpoch 10: val_accuracy improved from 0.85507 to 0.88043, saving model to qdense_best.hdf5\n21/21 [==============================] - 23s 1s/step - loss: 0.3872 - accuracy: 0.8333 - val_loss: 0.3372 - val_accuracy: 0.8804 - lr: 5.0000e-04\nEpoch 11/100\n21/21 [==============================] - ETA: 0s - loss: 0.3949 - accuracy: 0.8349\nEpoch 11: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3949 - accuracy: 0.8349 - val_loss: 0.3840 - val_accuracy: 0.8297 - lr: 5.0000e-04\nEpoch 12/100\n21/21 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.8287\nEpoch 12: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3935 - accuracy: 0.8287 - val_loss: 0.3696 - val_accuracy: 0.8333 - lr: 5.0000e-04\nEpoch 13/100\n21/21 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8380\nEpoch 13: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3997 - accuracy: 0.8380 - val_loss: 0.3525 - val_accuracy: 0.8623 - lr: 5.0000e-04\nEpoch 14/100\n21/21 [==============================] - ETA: 0s - loss: 0.3827 - accuracy: 0.8333\nEpoch 14: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3827 - accuracy: 0.8333 - val_loss: 0.3825 - val_accuracy: 0.8261 - lr: 5.0000e-04\nEpoch 15/100\n21/21 [==============================] - ETA: 0s - loss: 0.3978 - accuracy: 0.8396\nEpoch 15: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3978 - accuracy: 0.8396 - val_loss: 0.3348 - val_accuracy: 0.8732 - lr: 5.0000e-04\nEpoch 16/100\n21/21 [==============================] - ETA: 0s - loss: 0.3908 - accuracy: 0.8411\nEpoch 16: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3908 - accuracy: 0.8411 - val_loss: 0.3333 - val_accuracy: 0.8732 - lr: 5.0000e-04\nEpoch 17/100\n21/21 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8364\nEpoch 17: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3807 - accuracy: 0.8364 - val_loss: 0.3391 - val_accuracy: 0.8732 - lr: 5.0000e-04\nEpoch 18/100\n21/21 [==============================] - ETA: 0s - loss: 0.3759 - accuracy: 0.8411\nEpoch 18: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3759 - accuracy: 0.8411 - val_loss: 0.3874 - val_accuracy: 0.8152 - lr: 5.0000e-04\nEpoch 19/100\n21/21 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8411\nEpoch 19: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3896 - accuracy: 0.8411 - val_loss: 0.3397 - val_accuracy: 0.8623 - lr: 5.0000e-04\nEpoch 20/100\n21/21 [==============================] - ETA: 0s - loss: 0.3838 - accuracy: 0.8411\nEpoch 20: val_accuracy did not improve from 0.88043\n21/21 [==============================] - 23s 1s/step - loss: 0.3838 - accuracy: 0.8411 - val_loss: 0.3931 - val_accuracy: 0.8188 - lr: 5.0000e-04\nEpoch 21/100\n21/21 [==============================] - ETA: 0s - loss: 0.3902 - accuracy: 0.8287\nEpoch 21: val_accuracy improved from 0.88043 to 0.88768, saving model to qdense_best.hdf5\n21/21 [==============================] - 24s 1s/step - loss: 0.3902 - accuracy: 0.8287 - val_loss: 0.3224 - val_accuracy: 0.8877 - lr: 5.0000e-04\nEpoch 22/100\n21/21 [==============================] - ETA: 0s - loss: 0.3757 - accuracy: 0.8458\nEpoch 22: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3757 - accuracy: 0.8458 - val_loss: 0.3245 - val_accuracy: 0.8768 - lr: 5.0000e-04\nEpoch 23/100\n21/21 [==============================] - ETA: 0s - loss: 0.4022 - accuracy: 0.8333\nEpoch 23: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 24s 1s/step - loss: 0.4022 - accuracy: 0.8333 - val_loss: 0.3399 - val_accuracy: 0.8768 - lr: 5.0000e-04\nEpoch 24/100\n21/21 [==============================] - ETA: 0s - loss: 0.3979 - accuracy: 0.8411\nEpoch 24: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3979 - accuracy: 0.8411 - val_loss: 0.3256 - val_accuracy: 0.8804 - lr: 5.0000e-04\nEpoch 25/100\n21/21 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8427\nEpoch 25: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3749 - accuracy: 0.8427 - val_loss: 0.3333 - val_accuracy: 0.8732 - lr: 5.0000e-04\nEpoch 26/100\n21/21 [==============================] - ETA: 0s - loss: 0.3681 - accuracy: 0.8474\nEpoch 26: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3681 - accuracy: 0.8474 - val_loss: 0.3275 - val_accuracy: 0.8768 - lr: 5.0000e-04\nEpoch 27/100\n21/21 [==============================] - ETA: 0s - loss: 0.3786 - accuracy: 0.8380\nEpoch 27: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3786 - accuracy: 0.8380 - val_loss: 0.3308 - val_accuracy: 0.8804 - lr: 2.5000e-04\nEpoch 28/100\n21/21 [==============================] - ETA: 0s - loss: 0.3844 - accuracy: 0.8396\nEpoch 28: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3844 - accuracy: 0.8396 - val_loss: 0.3709 - val_accuracy: 0.8370 - lr: 2.5000e-04\nEpoch 29/100\n21/21 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8364\nEpoch 29: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3807 - accuracy: 0.8364 - val_loss: 0.3336 - val_accuracy: 0.8696 - lr: 2.5000e-04\nEpoch 30/100\n21/21 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.8411\nEpoch 30: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3791 - accuracy: 0.8411 - val_loss: 0.3224 - val_accuracy: 0.8804 - lr: 2.5000e-04\nEpoch 31/100\n21/21 [==============================] - ETA: 0s - loss: 0.3682 - accuracy: 0.8427\nEpoch 31: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3682 - accuracy: 0.8427 - val_loss: 0.3350 - val_accuracy: 0.8696 - lr: 2.5000e-04\nEpoch 32/100\n21/21 [==============================] - ETA: 0s - loss: 0.3828 - accuracy: 0.8411\nEpoch 32: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3828 - accuracy: 0.8411 - val_loss: 0.3353 - val_accuracy: 0.8696 - lr: 1.2500e-04\nEpoch 33/100\n21/21 [==============================] - ETA: 0s - loss: 0.3747 - accuracy: 0.8427\nEpoch 33: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 24s 1s/step - loss: 0.3747 - accuracy: 0.8427 - val_loss: 0.3287 - val_accuracy: 0.8732 - lr: 1.2500e-04\nEpoch 34/100\n21/21 [==============================] - ETA: 0s - loss: 0.3688 - accuracy: 0.8396\nEpoch 34: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3688 - accuracy: 0.8396 - val_loss: 0.3307 - val_accuracy: 0.8732 - lr: 1.2500e-04\nEpoch 35/100\n21/21 [==============================] - ETA: 0s - loss: 0.3647 - accuracy: 0.8474\nEpoch 35: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3647 - accuracy: 0.8474 - val_loss: 0.3278 - val_accuracy: 0.8732 - lr: 1.2500e-04\nEpoch 36/100\n21/21 [==============================] - ETA: 0s - loss: 0.3634 - accuracy: 0.8489\nEpoch 36: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3634 - accuracy: 0.8489 - val_loss: 0.3337 - val_accuracy: 0.8696 - lr: 1.2500e-04\nEpoch 37/100\n21/21 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.8458\nEpoch 37: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3654 - accuracy: 0.8458 - val_loss: 0.3349 - val_accuracy: 0.8696 - lr: 6.2500e-05\nEpoch 38/100\n21/21 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.8333\nEpoch 38: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 24s 1s/step - loss: 0.3798 - accuracy: 0.8333 - val_loss: 0.3324 - val_accuracy: 0.8732 - lr: 6.2500e-05\nEpoch 39/100\n21/21 [==============================] - ETA: 0s - loss: 0.3746 - accuracy: 0.8458\nEpoch 39: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 24s 1s/step - loss: 0.3746 - accuracy: 0.8458 - val_loss: 0.3255 - val_accuracy: 0.8768 - lr: 6.2500e-05\nEpoch 40/100\n21/21 [==============================] - ETA: 0s - loss: 0.3669 - accuracy: 0.8536\nEpoch 40: val_accuracy did not improve from 0.88768\n21/21 [==============================] - 23s 1s/step - loss: 0.3669 - accuracy: 0.8536 - val_loss: 0.3348 - val_accuracy: 0.8696 - lr: 6.2500e-05\n","output_type":"stream"}]},{"cell_type":"code","source":"best_model = model\nbest_model.evaluate(testX, testy)\nbest_model.evaluate(trainX, trainy)\npredy = best_model.predict(testX)\n\n# train_labels_pred= (predy = model.predict(trainX) > 0.5).astype(int).flatten()\ntrain_labels_real=trainy\nrounded_labels_pred= (predy > 0.5).astype(int).flatten()\nrounded_labels_real=testy\n\nlabels = ['Normal','Heart Disease']\nfrom sklearn.metrics import classification_report, roc_auc_score\n# print(classification_report(rounded_labels_real, rounded_labels_pred, target_names=labels))\nprint(classification_report(testy, rounded_labels_pred, target_names=labels))\n\nfrom sklearn.metrics import classification_report, matthews_corrcoef, cohen_kappa_score\n\n# Calculate MCC and Kappa scores\nmcc = matthews_corrcoef(testy, rounded_labels_pred)\nkappa = cohen_kappa_score(testy, rounded_labels_pred)\n\n# Print MCC and Kappa scores\nprint(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\nprint(f\"Cohen's Kappa Score: {kappa}\")\nprint(\"roc: \", roc_auc_score(rounded_labels_real, predy))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T20:47:46.000604Z","iopub.execute_input":"2024-08-13T20:47:46.000964Z","iopub.status.idle":"2024-08-13T20:48:01.540791Z","shell.execute_reply.started":"2024-08-13T20:47:46.000934Z","shell.execute_reply":"2024-08-13T20:48:01.539690Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"9/9 [==============================] - 4s 395ms/step - loss: 0.3224 - accuracy: 0.8804\n21/21 [==============================] - 8s 398ms/step - loss: 0.3633 - accuracy: 0.8427\n9/9 [==============================] - 3s 378ms/step\n               precision    recall  f1-score   support\n\n       Normal       0.84      0.88      0.86       112\nHeart Disease       0.91      0.88      0.90       164\n\n     accuracy                           0.88       276\n    macro avg       0.87      0.88      0.88       276\n weighted avg       0.88      0.88      0.88       276\n\nMatthews Correlation Coefficient (MCC): 0.7543361931280064\nCohen's Kappa Score: 0.7538112228349011\nroc:  0.9353222996515679\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**4 qubit 4 layer**","metadata":{}},{"cell_type":"code","source":"best_model = model\nbest_model.evaluate(testX, testy)\nbest_model.evaluate(trainX, trainy)\npredy = best_model.predict(testX)\n\n# train_labels_pred= (predy = model.predict(trainX) > 0.5).astype(int).flatten()\ntrain_labels_real=trainy\nrounded_labels_pred= (predy > 0.5).astype(int).flatten()\nrounded_labels_real=testy\n\nlabels = ['Normal','Heart Disease']\nfrom sklearn.metrics import classification_report, roc_auc_score\n# print(classification_report(rounded_labels_real, rounded_labels_pred, target_names=labels))\nprint(classification_report(testy, rounded_labels_pred, target_names=labels))\n\nfrom sklearn.metrics import classification_report, matthews_corrcoef, cohen_kappa_score\n\n# Calculate MCC and Kappa scores\nmcc = matthews_corrcoef(testy, rounded_labels_pred)\nkappa = cohen_kappa_score(testy, rounded_labels_pred)\n\n# Print MCC and Kappa scores\nprint(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\nprint(f\"Cohen's Kappa Score: {kappa}\")\nprint(\"roc: \", roc_auc_score(rounded_labels_real, predy))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:31:51.299016Z","iopub.execute_input":"2024-08-13T22:31:51.299611Z","iopub.status.idle":"2024-08-13T22:32:11.148618Z","shell.execute_reply.started":"2024-08-13T22:31:51.299567Z","shell.execute_reply":"2024-08-13T22:32:11.147464Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"9/9 [==============================] - 4s 498ms/step - loss: 0.3850 - accuracy: 0.8333\n21/21 [==============================] - 11s 514ms/step - loss: 0.4129 - accuracy: 0.8193\n9/9 [==============================] - 4s 482ms/step\n               precision    recall  f1-score   support\n\n       Normal       0.77      0.84      0.80       112\nHeart Disease       0.88      0.83      0.86       164\n\n     accuracy                           0.83       276\n    macro avg       0.83      0.83      0.83       276\n weighted avg       0.84      0.83      0.83       276\n\nMatthews Correlation Coefficient (MCC): 0.6610391110057477\nCohen's Kappa Score: 0.6592226755421946\nroc:  0.9075566202090593\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**4 qubit 3 layer**","metadata":{}},{"cell_type":"code","source":"best_model = model\nbest_model.evaluate(testX, testy)\nbest_model.evaluate(trainX, trainy)\npredy = best_model.predict(testX)\n\n# train_labels_pred= (predy = model.predict(trainX) > 0.5).astype(int).flatten()\ntrain_labels_real=trainy\nrounded_labels_pred= (predy > 0.5).astype(int).flatten()\nrounded_labels_real=testy\n\nlabels = ['Normal','Heart Disease']\nfrom sklearn.metrics import classification_report, roc_auc_score\n# print(classification_report(rounded_labels_real, rounded_labels_pred, target_names=labels))\nprint(classification_report(testy, rounded_labels_pred, target_names=labels))\n\nfrom sklearn.metrics import classification_report, matthews_corrcoef, cohen_kappa_score\n\n# Calculate MCC and Kappa scores\nmcc = matthews_corrcoef(testy, rounded_labels_pred)\nkappa = cohen_kappa_score(testy, rounded_labels_pred)\n\n# Print MCC and Kappa scores\nprint(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\nprint(f\"Cohen's Kappa Score: {kappa}\")\nprint(\"roc: \", roc_auc_score(rounded_labels_real, predy))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T22:13:57.007460Z","iopub.execute_input":"2024-08-13T22:13:57.007827Z","iopub.status.idle":"2024-08-13T22:14:15.125669Z","shell.execute_reply.started":"2024-08-13T22:13:57.007797Z","shell.execute_reply":"2024-08-13T22:14:15.124702Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"9/9 [==============================] - 4s 431ms/step - loss: 0.3818 - accuracy: 0.8188\n21/21 [==============================] - 9s 440ms/step - loss: 0.3951 - accuracy: 0.8240\n9/9 [==============================] - 4s 418ms/step\n               precision    recall  f1-score   support\n\n       Normal       0.75      0.82      0.79       112\nHeart Disease       0.87      0.82      0.84       164\n\n     accuracy                           0.82       276\n    macro avg       0.81      0.82      0.81       276\n weighted avg       0.82      0.82      0.82       276\n\nMatthews Correlation Coefficient (MCC): 0.6313246493383884\nCohen's Kappa Score: 0.6295898647197766\nroc:  0.9069577526132404\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**4 qubit 2 layer**","metadata":{}},{"cell_type":"code","source":"best_model = model\nbest_model.evaluate(testX, testy)\nbest_model.evaluate(trainX, trainy)\npredy = best_model.predict(testX)\n\n# train_labels_pred= (predy = model.predict(trainX) > 0.5).astype(int).flatten()\ntrain_labels_real=trainy\nrounded_labels_pred= (predy > 0.5).astype(int).flatten()\nrounded_labels_real=testy\n\nlabels = ['Normal','Heart Disease']\nfrom sklearn.metrics import classification_report, roc_auc_score\n# print(classification_report(rounded_labels_real, rounded_labels_pred, target_names=labels))\nprint(classification_report(testy, rounded_labels_pred, target_names=labels))\n\nfrom sklearn.metrics import classification_report, matthews_corrcoef, cohen_kappa_score\n\n# Calculate MCC and Kappa scores\nmcc = matthews_corrcoef(testy, rounded_labels_pred)\nkappa = cohen_kappa_score(testy, rounded_labels_pred)\n\n# Print MCC and Kappa scores\nprint(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\nprint(f\"Cohen's Kappa Score: {kappa}\")\nprint(\"roc: \", roc_auc_score(rounded_labels_real, predy))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T21:42:53.583575Z","iopub.execute_input":"2024-08-13T21:42:53.583951Z","iopub.status.idle":"2024-08-13T21:43:13.323666Z","shell.execute_reply.started":"2024-08-13T21:42:53.583921Z","shell.execute_reply":"2024-08-13T21:43:13.322634Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"9/9 [==============================] - 3s 356ms/step - loss: 0.3849 - accuracy: 0.8297\n21/21 [==============================] - 7s 356ms/step - loss: 0.3972 - accuracy: 0.8240\n9/9 [==============================] - 3s 344ms/step\n               precision    recall  f1-score   support\n\n       Normal       0.76      0.86      0.80       112\nHeart Disease       0.89      0.81      0.85       164\n\n     accuracy                           0.83       276\n    macro avg       0.82      0.83      0.83       276\n weighted avg       0.84      0.83      0.83       276\n\nMatthews Correlation Coefficient (MCC): 0.658247800429101\nCohen's Kappa Score: 0.6542275295873761\nroc:  0.9101154181184669\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**4 qubit 1 layer**","metadata":{}},{"cell_type":"code","source":"best_model = model\nbest_model.evaluate(testX, testy)\nbest_model.evaluate(trainX, trainy)\npredy = best_model.predict(testX)\n\n# train_labels_pred= (predy = model.predict(trainX) > 0.5).astype(int).flatten()\ntrain_labels_real=trainy\nrounded_labels_pred= (predy > 0.5).astype(int).flatten()\nrounded_labels_real=testy\n\nlabels = ['Normal','Heart Disease']\nfrom sklearn.metrics import classification_report, roc_auc_score\n# print(classification_report(rounded_labels_real, rounded_labels_pred, target_names=labels))\nprint(classification_report(testy, rounded_labels_pred, target_names=labels))\n\nfrom sklearn.metrics import classification_report, matthews_corrcoef, cohen_kappa_score\n\n# Calculate MCC and Kappa scores\nmcc = matthews_corrcoef(testy, rounded_labels_pred)\nkappa = cohen_kappa_score(testy, rounded_labels_pred)\n\n# Print MCC and Kappa scores\nprint(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\nprint(f\"Cohen's Kappa Score: {kappa}\")\nprint(\"roc: \", roc_auc_score(rounded_labels_real, predy))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T20:58:50.570446Z","iopub.execute_input":"2024-08-13T20:58:50.570845Z","iopub.status.idle":"2024-08-13T20:59:20.703357Z","shell.execute_reply.started":"2024-08-13T20:58:50.570814Z","shell.execute_reply":"2024-08-13T20:59:20.702256Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"9/9 [==============================] - 3s 295ms/step - loss: 0.3859 - accuracy: 0.8333\n21/21 [==============================] - 6s 284ms/step - loss: 0.3930 - accuracy: 0.8271\n9/9 [==============================] - 2s 272ms/step\n               precision    recall  f1-score   support\n\n       Normal       0.77      0.84      0.80       112\nHeart Disease       0.88      0.83      0.86       164\n\n     accuracy                           0.83       276\n    macro avg       0.83      0.83      0.83       276\n weighted avg       0.84      0.83      0.83       276\n\nMatthews Correlation Coefficient (MCC): 0.6610391110057477\nCohen's Kappa Score: 0.6592226755421946\nroc:  0.9057055749128919\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"NRmdUN8rvDHD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Js91KS4Ou4pf"},"execution_count":null,"outputs":[]}]}